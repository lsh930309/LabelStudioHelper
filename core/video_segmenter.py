#!/usr/bin/env python3
"""
ResNet ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ ë¹„ë””ì˜¤ ì„¸ê·¸ë©˜í…Œì´ì…˜
ë¹„ë””ì˜¤ë¥¼ ì˜ë¯¸ì  ìœ ì‚¬ë„ ê¸°ë°˜ìœ¼ë¡œ ë¶„í• í•˜ì—¬ ì •ì  êµ¬ê°„ ì œê±° ë° ë¼ë²¨ë§ íš¨ìœ¨ ê·¹ëŒ€í™”
ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ì‚¬ìš©í•œ í”„ë ˆì„ ê°„ ë¹„êµë¡œ ë” ì •í™•í•œ ì¥ë©´ ì „í™˜ ê°ì§€

ì‚¬ìš©ë²•:
    python tools/video_segmenter.py --input datasets/raw/gameplay.mp4 \
                                     --output datasets/clips/ \
                                     --mode auto \
                                     --use-gpu
"""

import argparse
import cv2
import numpy as np
from pathlib import Path
from typing import List, Tuple, Optional
from dataclasses import dataclass
import json
from datetime import datetime
from skimage.metrics import structural_similarity as ssim  # Legacy, ì‚¬ìš© ì•ˆí•¨
import subprocess
import shutil
import sys
import os
import bisect
import atexit
import signal
import gc
import time
import threading
import queue
from contextlib import contextmanager


def refresh_system_path():
    """
    ì‹œìŠ¤í…œ PATH í™˜ê²½ë³€ìˆ˜ë¥¼ ë ˆì§€ìŠ¤íŠ¸ë¦¬ì—ì„œ ìƒˆë¡œê³ ì¹¨ (Windows ì „ìš©)
    winget ì„¤ì¹˜ í›„ í˜„ì¬ í”„ë¡œì„¸ìŠ¤ì—ì„œ PATHë¥¼ ì¦‰ì‹œ ì‚¬ìš©í•˜ê¸° ìœ„í•¨
    """
    if sys.platform != 'win32':
        return

    try:
        import winreg
        import os

        # ì‹œìŠ¤í…œ PATH ì½ê¸°
        with winreg.OpenKey(winreg.HKEY_LOCAL_MACHINE,
                           r'SYSTEM\CurrentControlSet\Control\Session Manager\Environment',
                           0, winreg.KEY_READ) as key:
            system_path, _ = winreg.QueryValueEx(key, 'Path')

        # ì‚¬ìš©ì PATH ì½ê¸°
        with winreg.OpenKey(winreg.HKEY_CURRENT_USER,
                           r'Environment',
                           0, winreg.KEY_READ) as key:
            try:
                user_path, _ = winreg.QueryValueEx(key, 'Path')
            except FileNotFoundError:
                user_path = ''

        # í˜„ì¬ í”„ë¡œì„¸ìŠ¤ì˜ PATH ì—…ë°ì´íŠ¸
        combined_path = f"{user_path};{system_path}" if user_path else system_path
        os.environ['PATH'] = combined_path

    except Exception as e:
        # PATH ìƒˆë¡œê³ ì¹¨ ì‹¤íŒ¨ëŠ” ì¹˜ëª…ì ì´ì§€ ì•Šìœ¼ë¯€ë¡œ ë¬´ì‹œ
        pass


def check_and_install_ffmpeg() -> bool:
    """
    ffmpeg ì„¤ì¹˜ ì—¬ë¶€ í™•ì¸ ë° ìë™ ì„¤ì¹˜

    Returns:
        bool: ffmpegê°€ ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ True, ì„¤ì¹˜ ì‹¤íŒ¨í•˜ë©´ False
    """
    # ffmpegê°€ ì´ë¯¸ PATHì— ìˆëŠ”ì§€ í™•ì¸
    if shutil.which('ffmpeg') is not None:
        return True

    print("âš ï¸ ffmpegë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    print("ğŸ”§ ffmpegë¥¼ ìë™ìœ¼ë¡œ ì„¤ì¹˜í•©ë‹ˆë‹¤ (winget ì‚¬ìš©)...")

    try:
        # wingetìœ¼ë¡œ ffmpeg ì„¤ì¹˜ ì‹œë„
        result = subprocess.run(
            ['winget', 'install', 'Gyan.FFmpeg', '--accept-source-agreements', '--accept-package-agreements'],
            capture_output=True,
            text=True,
            timeout=300  # 5ë¶„ íƒ€ì„ì•„ì›ƒ
        )

        if result.returncode == 0:
            print("âœ… ffmpeg ì„¤ì¹˜ ì™„ë£Œ!")

            # PATH ìƒˆë¡œê³ ì¹¨ ì‹œë„
            print("ğŸ”„ PATH í™˜ê²½ë³€ìˆ˜ ìƒˆë¡œê³ ì¹¨ ì¤‘...")
            refresh_system_path()

            # ì„¤ì¹˜ í›„ ë‹¤ì‹œ í™•ì¸
            if shutil.which('ffmpeg') is not None:
                print("âœ… ffmpegë¥¼ ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!")
                return True
            else:
                print("   âš ï¸ ì„¤ì¹˜ëŠ” ì™„ë£Œë˜ì—ˆìœ¼ë‚˜ PATHì—ì„œ ffmpegë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                print("   â„¹ï¸ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¥¼ ì‹œë„í•´ì£¼ì„¸ìš”:")
                print("      1. í„°ë¯¸ë„ì„ ì¬ì‹œì‘í•œ í›„ ë‹¤ì‹œ ì‹¤í–‰")
                print("      2. ì‹œìŠ¤í…œì„ ì¬ë¶€íŒ…")
                return False
        else:
            print(f"âŒ ffmpeg ì„¤ì¹˜ ì‹¤íŒ¨")
            if result.stderr:
                print(f"   ì˜¤ë¥˜ ë©”ì‹œì§€: {result.stderr[:200]}")
            print("   â„¹ï¸ ìˆ˜ë™ ì„¤ì¹˜ ë°©ë²•:")
            print("      1. í„°ë¯¸ë„ì—ì„œ 'winget install Gyan.FFmpeg' ì‹¤í–‰")
            print("      2. ë˜ëŠ” https://www.gyan.dev/ffmpeg/builds/ ì—ì„œ ë‹¤ìš´ë¡œë“œ")
            return False

    except FileNotFoundError:
        print("âŒ wingetì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("   â„¹ï¸ Windows 10 1809 ì´ìƒ ë˜ëŠ” Windows 11ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        print("   â„¹ï¸ ìˆ˜ë™ ì„¤ì¹˜: https://www.gyan.dev/ffmpeg/builds/")
        return False
    except subprocess.TimeoutExpired:
        print("âŒ ffmpeg ì„¤ì¹˜ ì‹œê°„ ì´ˆê³¼ (5ë¶„)")
        print("   â„¹ï¸ ë„¤íŠ¸ì›Œí¬ ìƒíƒœë¥¼ í™•ì¸í•˜ê³  ìˆ˜ë™ìœ¼ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
        return False
    except Exception as e:
        print(f"âŒ ffmpeg ì„¤ì¹˜ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        return False


class GPUResourceManager:
    """
    GPU ë¦¬ì†ŒìŠ¤ ìë™ ê´€ë¦¬ Context Manager
    ì‘ì—… ì™„ë£Œ/ì·¨ì†Œ/ì˜¤ë¥˜ ì‹œ ìë™ìœ¼ë¡œ VRAM ì •ë¦¬
    """
    def __init__(self, device=None):
        self.device = device
        self.is_cuda = device and device.type == 'cuda'

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """ë¬´ì¡°ê±´ VRAM ì •ë¦¬ (ì •ìƒ ì¢…ë£Œ/ì˜ˆì™¸ ëª¨ë‘)"""
        if self.is_cuda:
            try:
                import torch
                print("\nğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘...")
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
                gc.collect()

                memory_allocated = torch.cuda.memory_allocated(0) / 1024 / 1024
                memory_reserved = torch.cuda.memory_reserved(0) / 1024 / 1024
                print(f"   GPU ë©”ëª¨ë¦¬ í• ë‹¹: {memory_allocated:.1f} MB")
                print(f"   GPU ë©”ëª¨ë¦¬ ì˜ˆì•½: {memory_reserved:.1f} MB")
                print(f"   âœ… GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
            except Exception as e:
                print(f"   âš ï¸ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨ (ë¬´ì‹œë¨): {e}")
        return False  # ì˜ˆì™¸ë¥¼ ë‹¤ì‹œ ë°œìƒì‹œí‚´


def extract_keyframes(video_path: Path) -> List[float]:
    """
    ë¹„ë””ì˜¤ì˜ ëª¨ë“  I-Frame(Keyframe) íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ
    
    1. ê³ ì† ëª¨ë“œ (Packet Header): ì»¨í…Œì´ë„ˆì˜ íŒ¨í‚· í”Œë˜ê·¸ë§Œ í™•ì¸ (ë§¤ìš° ë¹ ë¦„)
    2. ì •ë°€ ëª¨ë“œ (Frame Decode): ì‹¤ì œ í”„ë ˆì„ ë””ì½”ë”© (ëŠë¦¼, ê³ ì† ëª¨ë“œ ì‹¤íŒ¨ ì‹œ Fallback)

    Args:
        video_path: ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ

    Returns:
        Keyframe íƒ€ì„ìŠ¤íƒ¬í”„ ë¦¬ìŠ¤íŠ¸ (ì´ˆ ë‹¨ìœ„, ì •ë ¬ë¨)
    """
    print("ğŸ” Keyframe ì¸ë±ì‹± ì‹œì‘ (ê³ ì† ëª¨ë“œ)...")
    
    # 1. ê³ ì† ëª¨ë“œ ì‹œë„ (Packet)
    try:
        cmd = [
            'ffprobe',
            '-select_streams', 'v:0',
            '-show_entries', 'packet=pts_time,flags',
            '-of', 'csv=print_section=0',
            str(video_path)
        ]

        process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            creationflags=subprocess.CREATE_NO_WINDOW if os.name == 'nt' else 0,
            encoding='utf-8',
            errors='replace'
        )

        keyframes = []
        last_log_time = time.time()
        
        while True:
            line = process.stdout.readline()
            if not line and process.poll() is not None:
                break
            
            if not line:
                continue

            # pts_time, flags (e.g., "12.345,K__")
            parts = line.strip().split(',')
            if len(parts) >= 2:
                pts_time, flags = parts[0], parts[1]
                # 'K' í”Œë˜ê·¸ê°€ ìˆìœ¼ë©´ Keyframe
                if 'K' in flags and pts_time != 'N/A':
                    try:
                        keyframes.append(float(pts_time))
                    except ValueError:
                        continue
            
            current_time = time.time()
            if current_time - last_log_time > 5.0:
                print(f"   í‚¤í”„ë ˆì„ ì¸ë±ì‹± ì¤‘ (ê³ ì†)... ({len(keyframes)}ê°œ ë°œê²¬)", flush=True)
                last_log_time = current_time

        process.wait(timeout=60) # íŒ¨í‚· ìŠ¤ìº”ì€ ë§¤ìš° ë¹ ë¥´ë¯€ë¡œ íƒ€ì„ì•„ì›ƒ ì§§ê²Œ

        if process.returncode == 0 and keyframes:
            keyframes.sort()
            print(f"âœ… Keyframe {len(keyframes)}ê°œ ì¸ë±ì‹± ì™„ë£Œ (ê³ ì† ëª¨ë“œ)", flush=True)
            return keyframes
        
        print("âš ï¸ ê³ ì† ëª¨ë“œ ì¸ë±ì‹± ì‹¤íŒ¨ ë˜ëŠ” í‚¤í”„ë ˆì„ ì—†ìŒ. ì •ë°€ ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.")

    except Exception as e:
        print(f"âš ï¸ ê³ ì† ëª¨ë“œ ì˜¤ë¥˜: {e}. ì •ë°€ ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
        if 'process' in locals() and process:
            try:
                process.kill()
            except:
                pass

    # 2. ì •ë°€ ëª¨ë“œ (Fallback)
    return _extract_keyframes_deep_scan(video_path)


def _extract_keyframes_deep_scan(video_path: Path) -> List[float]:
    """
    ì •ë°€ ëª¨ë“œ: ffprobe frame ë””ì½”ë”©ì„ í†µí•œ Keyframe ì¶”ì¶œ (ëŠë¦¼)
    """
    print("ğŸ” Keyframe ì¸ë±ì‹± ì‹œì‘ (ì •ë°€ ëª¨ë“œ - ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)...")
    try:
        cmd = [
            'ffprobe',
            '-select_streams', 'v:0',
            '-show_entries', 'frame=pts_time,pict_type',
            '-of', 'csv=print_section=0',
            str(video_path)
        ]

        # Popenìœ¼ë¡œ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰ (ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥)
        process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            creationflags=subprocess.CREATE_NO_WINDOW if os.name == 'nt' else 0,
            encoding='utf-8',
            errors='replace'
        )

        keyframes = []
        last_log_time = time.time()
        
        # stdout ë¼ì¸ë³„ ì½ê¸°
        while True:
            line = process.stdout.readline()
            if not line and process.poll() is not None:
                break
            
            if not line:
                continue

            parts = line.strip().split(',')
            if len(parts) >= 2:
                pts_time, pict_type = parts[0], parts[1]
                if pict_type == 'I' and pts_time != 'N/A':
                    try:
                        keyframes.append(float(pts_time))
                    except ValueError:
                        continue
            
            # 5ì´ˆë§ˆë‹¤ ì§„í–‰ ìƒí™© ë¡œê¹…
            current_time = time.time()
            if current_time - last_log_time > 5.0:
                print(f"   í‚¤í”„ë ˆì„ ì¸ë±ì‹± ì¤‘ (ì •ë°€)... ({len(keyframes)}ê°œ ë°œê²¬)", flush=True)
                last_log_time = current_time

        # íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬ (300ì´ˆ)
        try:
            process.wait(timeout=300)
        except subprocess.TimeoutExpired:
            process.kill()
            print("âš ï¸ Keyframe ì¶”ì¶œ ì‹œê°„ ì´ˆê³¼ (5ë¶„), ë¶€ë¶„ ê²°ê³¼ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        
        if process.returncode != 0 and process.returncode is not None:
             # stderr ì½ê¸°
            stderr_output = process.stderr.read()
            print(f"âš ï¸ Keyframe ì¶”ì¶œ ê²½ê³  (ffprobe): {stderr_output[:200]}")

        keyframes.sort()
        print(f"âœ… Keyframe {len(keyframes)}ê°œ ì¸ë±ì‹± ì™„ë£Œ (ì •ë°€ ëª¨ë“œ)", flush=True)
        return keyframes

    except Exception as e:
        print(f"âš ï¸ Keyframe ì¶”ì¶œ ì‹¤íŒ¨: {e}, Keyframe ì •ë ¬ ë¹„í™œì„±í™”")
        return []


def snap_to_keyframe(time: float, keyframes: List[float], direction: str = 'before') -> float:
    """
    ì£¼ì–´ì§„ ì‹œê°„ì„ ê°€ì¥ ê°€ê¹Œìš´ Keyframeìœ¼ë¡œ ì •ë ¬

    Args:
        time: ëŒ€ìƒ ì‹œê°„ (ì´ˆ)
        keyframes: Keyframe íƒ€ì„ìŠ¤íƒ¬í”„ ë¦¬ìŠ¤íŠ¸ (ì •ë ¬ë¨)
        direction: 'before' (ì´ì „ keyframe) ë˜ëŠ” 'after' (ë‹¤ìŒ keyframe)

    Returns:
        ì •ë ¬ëœ ì‹œê°„ (ì´ˆ)
    """
    if not keyframes:
        return time

    idx = bisect.bisect_left(keyframes, time)

    if direction == 'before':
        # ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ê²½ìš° í•´ë‹¹ Keyframe ë°˜í™˜
        if idx < len(keyframes) and abs(keyframes[idx] - time) < 1e-5:
            return keyframes[idx]
            
        # ì´ì „ Keyframe
        if idx == 0:
            return keyframes[0]
        return keyframes[idx - 1]
    else:
        # ë‹¤ìŒ Keyframe
        if idx >= len(keyframes):
            return keyframes[-1]
        return keyframes[idx]


class PyAVVideoReader:
    """
    PyAVë¥¼ ì‚¬ìš©í•œ ë¹„ë””ì˜¤ ë¦¬ë” (OpenCVê°€ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì½”ë± ì²˜ë¦¬)
    AV1, H.265, VP9 ë“± ëª¨ë“  FFmpeg ì§€ì› ì½”ë±ì„ ë¬´ì†ì‹¤ë¡œ ì½ì„ ìˆ˜ ìˆìŒ
    """

    def __init__(self, video_path: Path):
        """
        Args:
            video_path: ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        """
        self.video_path = video_path
        self.container = None
        self.video_stream = None
        self.fps = None
        self.total_frames = None
        self.width = None
        self.height = None
        self._frame_generator = None

    def open(self) -> bool:
        """
        ë¹„ë””ì˜¤ íŒŒì¼ ì—´ê¸°

        Returns:
            bool: ì„±ê³µ ì—¬ë¶€
        """
        try:
            import av

            self.container = av.open(str(self.video_path))
            self.video_stream = self.container.streams.video[0]

            # ë¹„ë””ì˜¤ ì •ë³´ ì¶”ì¶œ
            self.fps = float(self.video_stream.average_rate)
            self.total_frames = self.video_stream.frames
            self.width = self.video_stream.width
            self.height = self.video_stream.height

            # total_framesê°€ 0ì´ë©´ durationìœ¼ë¡œ ì¶”ì •
            if self.total_frames == 0 and self.container.duration:
                self.total_frames = int(self.container.duration * self.fps / av.time_base)

            # í”„ë ˆì„ ì œë„ˆë ˆì´í„° ì´ˆê¸°í™”
            self._frame_generator = self.container.decode(video=0)

            print(f"âœ… PyAVë¡œ ë¹„ë””ì˜¤ ì—´ê¸° ì„±ê³µ")
            print(f"   ì½”ë±: {self.video_stream.codec_context.name}")

            return True

        except ImportError:
            print("âš ï¸ PyAVê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("   ì„¤ì¹˜ ë°©ë²•: pip install av")
            return False
        except Exception as e:
            print(f"âš ï¸ PyAVë¡œ ë¹„ë””ì˜¤ ì—´ê¸° ì‹¤íŒ¨: {e}")
            return False

    def read(self):
        """
        ë‹¤ìŒ í”„ë ˆì„ ì½ê¸° (OpenCVì˜ cap.read()ì™€ ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤)

        Returns:
            tuple: (success, frame_bgr) - OpenCV í˜•ì‹ì˜ BGR numpy array
        """
        try:
            frame = next(self._frame_generator)
            # BGR í¬ë§·ìœ¼ë¡œ ë³€í™˜ (OpenCV í˜¸í™˜)
            img = frame.to_ndarray(format='bgr24')
            return True, img
        except StopIteration:
            return False, None
        except Exception as e:
            return False, None

    def grab(self):
        """
        í”„ë ˆì„ì„ ê±´ë„ˆë›°ê¸° (OpenCVì˜ cap.grab()ì™€ ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤)
        PyAVëŠ” grabì„ ì§ì ‘ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ readí•˜ê³  ë²„ë¦¼

        Returns:
            bool: ì„±ê³µ ì—¬ë¶€
        """
        try:
            frame = next(self._frame_generator)
            return True
        except StopIteration:
            return False
        except Exception as e:
            return False

    def release(self):
        """ë¦¬ì†ŒìŠ¤ í•´ì œ"""
        if self.container:
            self.container.close()
            self.container = None

    def isOpened(self) -> bool:
        """ë¹„ë””ì˜¤ê°€ ì—´ë ¤ìˆëŠ”ì§€ í™•ì¸"""
        return self.container is not None


class FrameReader(threading.Thread):
    """
    ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ í”„ë ˆì„ì„ ë¯¸ë¦¬ ì½ì–´ì˜¤ëŠ” í´ë˜ìŠ¤ (I/O ë³‘ëª© í•´ê²°)
    Producer-Consumer íŒ¨í„´ ì‚¬ìš©
    """
    def __init__(self, cap, queue_size=64):
        super().__init__()
        self.cap = cap
        self.queue = queue.Queue(maxsize=queue_size)
        self.stop_event = threading.Event()
        self.daemon = True  # ë©”ì¸ ìŠ¤ë ˆë“œ ì¢…ë£Œ ì‹œ ìë™ ì¢…ë£Œ

    def run(self):
        """í”„ë ˆì„ ì½ê¸° ë£¨í”„ (Producer)"""
        while not self.stop_event.is_set():
            try:
                ret, frame = self.cap.read()
                if not ret:
                    # EOF ë„ë‹¬ ì‹œ Noneì„ ë„£ì–´ ì•Œë¦¼
                    self.queue.put((False, None))
                    break
                
                # íì— ë„£ê¸° (ê½‰ ì°¨ë©´ ëŒ€ê¸°)
                self.queue.put((True, frame))
            except Exception as e:
                print(f"âš ï¸ FrameReader ì˜¤ë¥˜: {e}")
                self.queue.put((False, None))
                break

    def read(self):
        """í”„ë ˆì„ ê°€ì ¸ì˜¤ê¸° (Consumer)"""
        try:
            # íì—ì„œ ê°€ì ¸ì˜¤ê¸° (íƒ€ì„ì•„ì›ƒ 5ì´ˆ)
            return self.queue.get(timeout=5)
        except queue.Empty:
            return False, None

    def stop(self):
        """ìŠ¤ë ˆë“œ ì¢…ë£Œ ìš”ì²­"""
        self.stop_event.set()
        # í ë¹„ìš°ê¸° (ë°ë“œë½ ë°©ì§€)
        while not self.queue.empty():
            try:
                self.queue.get_nowait()
            except queue.Empty:
                break


@dataclass
class VideoSegment:
    """ë¹„ë””ì˜¤ ì„¸ê·¸ë¨¼íŠ¸ ì •ë³´"""
    start_frame: int
    end_frame: int
    start_time: float
    end_time: float
    duration: float
    avg_ssim: float  # êµ¬ê°„ ë‚´ í‰ê·  SSIM (ì•ˆì •ì„± ì§€í‘œ)
    intervals: Optional[List[Tuple[float, float]]] = None  # Virtual Timeline: ì—¬ëŸ¬ êµ¬ê°„ [(start, end), ...]


@dataclass
class SegmentConfig:
    """ì„¸ê·¸ë©˜í…Œì´ì…˜ ì„¤ì •"""
    # ëª¨ë“œ ì„¤ì •
    mode: str = "custom"                 # "custom" ê³ ì •

    # ì •ì  êµ¬ê°„ ê°ì§€ (ResNet ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ì¤€)
    static_threshold: float = 0.95       # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ê°€ ì´ë³´ë‹¤ ë†’ìœ¼ë©´ ë„ˆë¬´ ì •ì  (ì œì™¸)
    min_static_duration: float = 1.0     # ìµœì†Œ ì •ì  êµ¬ê°„ ê¸¸ì´ (ì´ˆ)

    # ì¶œë ¥ ì„¸ê·¸ë¨¼íŠ¸ ì„¤ì •
    target_segment_duration: float = 30.0  # ëª©í‘œ ì„¸ê·¸ë¨¼íŠ¸ ê¸¸ì´ (ì´ˆ)

    # ResNet Feature ì¶”ì¶œ ì„¤ì •
    feature_sample_rate: int = 1         # Ní”„ë ˆì„ë§ˆë‹¤ feature ì¶”ì¶œ (ì„±ëŠ¥ ìµœì í™”)
    enable_visualization: bool = True    # ìœ ì‚¬ë„ ê·¸ë˜í”„ ì¶œë ¥ ì—¬ë¶€

    # ì„±ëŠ¥ ìµœì í™” (deprecated, í˜¸í™˜ì„± ìœ ì§€)
    ssim_scale: float = 1.0              # (ì‚¬ìš© ì•ˆ í•¨, í˜¸í™˜ì„± ìœ ì§€)
    frame_skip: int = 1                  # (ì‚¬ìš© ì•ˆ í•¨, feature_sample_rateë¡œ ëŒ€ì²´ë¨)
    use_gpu: bool = False                # GPU ê°€ì† ì‚¬ìš© (CUDA í•„ìˆ˜)
    initial_batch_size: int = 128        # ì´ˆê¸° ë°°ì¹˜ í¬ê¸° (ë™ì  ì¡°ì •ë¨)
    max_vram_usage: float = 0.85         # ìµœëŒ€ VRAM ì‚¬ìš©ë¥  (85%)

    # ì‹¤í—˜ ê¸°ëŠ¥
    save_discarded: bool = False         # ì±„íƒë˜ì§€ ì•Šì€ êµ¬ê°„ë„ ë³„ë„ ì €ì¥
    enable_keyframe_snap: bool = True    # Keyframe ì •ë ¬ í™œì„±í™”

    # ì¸ì½”ë”© ì„¤ì •
    re_encode: bool = False              # ì¬ì¸ì½”ë”© ì—¬ë¶€ (True=ì¬ì¸ì½”ë”©, False=ìŠ¤íŠ¸ë¦¼ ë³µì‚¬)
    encode_quality: int = 0             # ì¸ì½”ë”© í’ˆì§ˆ (CRF, 0~51, ë‚®ì„ìˆ˜ë¡ ê³ í™”ì§ˆ)
    encode_preset: str = 'fast'          # ì¸ì½”ë”© í”„ë¦¬ì…‹ (ultrafast ~ veryslow)


class VideoSegmenter:
    """ResNet ê¸°ë°˜ ë¹„ë””ì˜¤ ì„¸ê·¸ë©˜í…Œì´ì…˜ (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)"""

    def __init__(self, config: SegmentConfig = None):
        self.config = config or SegmentConfig()
        self.stats = {
            'total_frames': 0,
            'static_segments_removed': 0,     # ì œê±°ëœ ì •ì  êµ¬ê°„ ìˆ˜
            'output_segments': 0,             # ìµœì¢… ì¶œë ¥ ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜
            'similarity_gpu_count': 0,        # GPUë¡œ ê³„ì‚°í•œ ìœ ì‚¬ë„ íšŸìˆ˜
            'similarity_cpu_count': 0,        # CPUë¡œ ê³„ì‚°í•œ ìœ ì‚¬ë„ íšŸìˆ˜
            'similarity_gpu_time': 0.0,       # GPU ìœ ì‚¬ë„ ì´ ì‹œê°„ (ì´ˆ)
            'similarity_cpu_time': 0.0,       # CPU ìœ ì‚¬ë„ ì´ ì‹œê°„ (ì´ˆ)
            'batch_size_adjustments': 0,      # ë°°ì¹˜ í¬ê¸° ì¡°ì • íšŸìˆ˜
        }

        # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        self.gpu_available = False
        self.device = None
        self.current_batch_size = self.config.initial_batch_size
        self.keyframes = []  # Keyframe íƒ€ì„ìŠ¤íƒ¬í”„ ìºì‹œ
        self.feature_extractor = None  # ResNet Feature Extractor
        self.similarity_scores_cache = []  # ìœ ì‚¬ë„ ì ìˆ˜ ìºì‹œ (ì‹œê°í™”ìš©)

        if self.config.use_gpu:
            self.gpu_available = self._check_gpu_available()

        # Feature Extractor ì´ˆê¸°í™”
        if self.gpu_available:
            try:
                from tools.feature_extractor import FeatureExtractor
                self.feature_extractor = FeatureExtractor(device=self.device, use_fp16=True)
            except Exception as e:
                print(f"âš ï¸ Feature Extractor ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                print("   CPU ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
                self.gpu_available = False

        # ì´ˆê¸° ìƒíƒœ ì¶œë ¥
        self._print_initial_status()

        # atexit/signal handler ë“±ë¡ (VRAM ì •ë¦¬)
        self._register_cleanup_handlers()

    def _print_initial_status(self):
        """ì´ˆê¸° ì„¤ì • ë° ìƒíƒœ ì¶œë ¥"""
        print("=" * 60)
        print(f"ğŸ¥ ë¹„ë””ì˜¤ ì„¸ê·¸ë©˜í„° ì´ˆê¸°í™” (ResNet ê¸°ë°˜)")
        print(f"   - ëª¨ë“œ: {self.config.mode.upper()}")
        print(f"   - GPU ê°€ì†: {'í™œì„±í™”' if self.gpu_available else 'ë¹„í™œì„±í™”'}")
        if self.gpu_available:
            import torch
            print(f"     â€¢ ì¥ì¹˜: {torch.cuda.get_device_name(0)}")
            print(f"     â€¢ VRAM ì œí•œ: {self.config.max_vram_usage * 100:.0f}%")

        print(f"   - ì„¤ì •:")
        print(f"     â€¢ ì •ì  ì„ê³„ê°’: {self.config.static_threshold}")
        print(f"     â€¢ ìµœì†Œ ì •ì  ê¸¸ì´: {self.config.min_static_duration}ì´ˆ")
        print(f"     â€¢ ëª©í‘œ ì„¸ê·¸ë¨¼íŠ¸: {self.config.target_segment_duration}ì´ˆ")
        print(f"     â€¢ Feature ìƒ˜í”Œë§: {self.config.feature_sample_rate}í”„ë ˆì„ë§ˆë‹¤")
        print(f"     â€¢ ì‹œê°í™”: {'í™œì„±í™”' if self.config.enable_visualization else 'ë¹„í™œì„±í™”'}")
        print("=" * 60, flush=True)


    def _register_cleanup_handlers(self):
        """atexit ë° signal handler ë“±ë¡"""
        def cleanup():
            if self.gpu_available and self.device and self.device.type == 'cuda':
                try:
                    import torch
                    torch.cuda.empty_cache()
                    gc.collect()
                except:
                    pass

        atexit.register(cleanup)

        # Windowsì—ì„œëŠ” SIGBREAK, Unixì—ì„œëŠ” SIGINT/SIGTERM
        if sys.platform == 'win32':
            try:
                signal.signal(signal.SIGBREAK, lambda sig, frame: cleanup())
            except:
                pass
        else:
            signal.signal(signal.SIGINT, lambda sig, frame: cleanup())
            signal.signal(signal.SIGTERM, lambda sig, frame: cleanup())

    def _check_gpu_available(self) -> bool:
        """
        GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸ (CUDA/PyTorch)

        Returns:
            bool: GPU ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ True
        """
        try:
            # 1. PyTorch ì„¤ì¹˜ ê²½ë¡œë¥¼ sys.pathì— ì¶”ê°€
            import sys
            from pathlib import Path
            import os

            # src/utils ê²½ë¡œ ì¶”ê°€ (PyTorchInstaller importìš©)
            if getattr(sys, 'frozen', False):
                # PyInstaller íŒ¨í‚¤ì§• í™˜ê²½
                utils_dir = Path(sys.executable).parent / "_internal" / "src"
            else:
                # ê°œë°œ í™˜ê²½
                script_dir = Path(__file__).parent.parent
                utils_dir = script_dir / "src"

            if utils_dir.exists() and str(utils_dir) not in sys.path:
                sys.path.insert(0, str(utils_dir))

            # 2. PyTorch ì„¤ì¹˜ í™•ì¸
            try:
                from utils.pytorch_installer import PyTorchInstaller
                installer = PyTorchInstaller.get_instance()

                if not installer.is_pytorch_installed():
                    print("âš ï¸ PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
                    print("   GPU ê°€ì†ì„ ì‚¬ìš©í•˜ë ¤ë©´ GUIì—ì„œ 'GPU ê°€ì†' ì²´í¬ë°•ìŠ¤ë¥¼ í™œì„±í™”í•˜ì„¸ìš”.")
                    return False

                # sys.pathì— PyTorch ê²½ë¡œ ì¶”ê°€
                installer.add_to_path()

            except ImportError:
                # PyTorchInstallerë¥¼ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš° (ì´ì „ ë²„ì „ í˜¸í™˜ì„±)
                print("âš ï¸ PyTorchInstallerë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‹œìŠ¤í…œ PyTorchë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")

            # 3. PyTorch import ì‹œë„
            import torch

            if torch.cuda.is_available():
                self.device = torch.device('cuda')
                gpu_name = torch.cuda.get_device_name(0)
                print(f"âœ… GPU ê°ì§€ë¨: {gpu_name}")

                # 4. ì‹¤ì œ GPU í…ì„œ ìƒì„± ë° ì—°ì‚° í…ŒìŠ¤íŠ¸
                print("ğŸ” GPU ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸ ì¤‘...")
                try:
                    # ì‘ì€ í–‰ë ¬ ê³±ì…ˆìœ¼ë¡œ GPU ì‘ë™ í™•ì¸
                    test_tensor = torch.randn(100, 100, device=self.device)
                    result = test_tensor @ test_tensor.T
                    torch.cuda.synchronize()  # GPU ì‘ì—… ì™„ë£Œ ëŒ€ê¸°

                    # ë©”ëª¨ë¦¬ ì •ë³´ í™•ì¸
                    memory_allocated = torch.cuda.memory_allocated(0) / 1024 / 1024  # MB
                    memory_reserved = torch.cuda.memory_reserved(0) / 1024 / 1024    # MB

                    print(f"âœ… GPU ê°€ì† í™œì„±í™” ì„±ê³µ!")
                    print(f"   - GPU ë©”ëª¨ë¦¬ í• ë‹¹: {memory_allocated:.1f} MB")
                    print(f"   - GPU ë©”ëª¨ë¦¬ ì˜ˆì•½: {memory_reserved:.1f} MB")
                    return True

                except RuntimeError as e:
                    print(f"âŒ GPU í…ì„œ ìƒì„± ì‹¤íŒ¨: {e}")
                    print("   CPU ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
                    return False
                except Exception as e:
                    print(f"âŒ GPU ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
                    print("   CPU ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
                    return False
            else:
                print("âš ï¸ CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
                return False

        except ImportError:
            print("âš ï¸ PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
            print("   GPU ê°€ì†ì„ ì‚¬ìš©í•˜ë ¤ë©´ GUIì—ì„œ 'GPU ê°€ì†' ì²´í¬ë°•ìŠ¤ë¥¼ í™œì„±í™”í•˜ì„¸ìš”.")
            return False
        except (OSError, RuntimeError) as e:
            # DLL ë¡œë”© ì‹¤íŒ¨ ë˜ëŠ” CUDA ì´ˆê¸°í™” ì˜¤ë¥˜
            print(f"âš ï¸ PyTorch ë¡œë”© ì‹¤íŒ¨: {e}")
            print("   CPU ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
            print("   í•´ê²° ë°©ë²•:")
            print("   1. GUIì—ì„œ 'GPU ê°€ì†' ì²´í¬ë°•ìŠ¤ë¥¼ ë‹¤ì‹œ í™œì„±í™”í•˜ì„¸ìš”.")
            print("   2. PyTorch ì¬ì„¤ì¹˜: ì„¤ì • ë©”ë‰´ì—ì„œ 'PyTorch ì¬ì„¤ì¹˜' í´ë¦­")
            return False
        except Exception as e:
            # ê¸°íƒ€ ëª¨ë“  ì˜ˆì™¸
            print(f"âš ï¸ GPU ì´ˆê¸°í™” ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
            print("   CPU ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
            return False

    def calculate_similarity(self, img1: np.ndarray, img2: np.ndarray) -> float:
        """
        ë‘ í”„ë ˆì„ ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (ResNet ê¸°ë°˜)

        1. ì´ë¯¸ì§€ â†’ ResNet feature ì¶”ì¶œ (512ì°¨ì›)
        2. L2 ì •ê·œí™”
        3. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ = ë‚´ì 

        Returns:
            ìœ ì‚¬ë„ (0~1, ë†’ì„ìˆ˜ë¡ ìœ ì‚¬)
        """
        # GPU ê°€ì† í•„ìˆ˜ (Feature Extractor ì‚¬ìš©)
        if self.feature_extractor is not None:
            import time
            start_time = time.perf_counter()

            # Feature ì¶”ì¶œ (ë°°ì¹˜ í¬ê¸° 2)
            features = self.feature_extractor.extract_frame_features([img1, img2])

            if len(features) >= 2:
                # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                score = self.feature_extractor.calculate_cosine_similarity(features[0], features[1])
                elapsed = time.perf_counter() - start_time

                self.stats['similarity_gpu_count'] += 1
                self.stats['similarity_gpu_time'] += elapsed
                return score

        # GPU ì‚¬ìš© ë¶ˆê°€ ì‹œ ì˜¤ë¥˜ ë°œìƒ
        raise RuntimeError("GPU Feature Extractorë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. GPU ê°€ì†ì„ í™œì„±í™”í•´ì£¼ì„¸ìš”.")

    def _adjust_batch_size(self, error: Exception = None):
        """
        VRAM ì‚¬ìš©ëŸ‰ì— ë”°ë¼ ë™ì ìœ¼ë¡œ ë°°ì¹˜ í¬ê¸° ì¡°ì •

        Args:
            error: OOM ì—ëŸ¬ê°€ ë°œìƒí•œ ê²½ìš° ì „ë‹¬
        """
        if not self.gpu_available:
            return

        try:
            import torch

            # OOM ë°œìƒ ì‹œ ë°°ì¹˜ í¬ê¸° ê°ì†Œ
            if error and isinstance(error, torch.cuda.OutOfMemoryError):
                self.current_batch_size = max(1, self.current_batch_size // 2)
                self.stats['batch_size_adjustments'] += 1
                torch.cuda.empty_cache()
                print(f"âš ï¸ GPU ë©”ëª¨ë¦¬ ë¶€ì¡±, ë°°ì¹˜ í¬ê¸° ê°ì†Œ: {self.current_batch_size * 2} â†’ {self.current_batch_size}")
                return

            # ê°€ìš© VRAM ì²´í¬ ë° ë°°ì¹˜ í¬ê¸° ì¡°ì •
            free_mem, total_mem = torch.cuda.mem_get_info(0)
            free_ratio = free_mem / total_mem
            
            # 1. ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ë°°ì¹˜ í¬ê¸° ê°ì†Œ (Proactive)
            # ë‚¨ì€ ë©”ëª¨ë¦¬ê°€ 25% ë¯¸ë§Œì´ë©´ ë°°ì¹˜ í¬ê¸° ê°ì†Œ
            if free_ratio < (1 - self.config.max_vram_usage):
                if self.current_batch_size > 1:
                    old_size = self.current_batch_size
                    self.current_batch_size = max(1, self.current_batch_size // 2)
                    self.stats['batch_size_adjustments'] += 1
                    torch.cuda.empty_cache()  # ì¤‘ìš”: ìºì‹œ ë¹„ìš°ê¸°
                    print(f"âš ï¸ VRAM ì—¬ìœ  ë¶€ì¡± ({free_ratio*100:.1f}%), ë°°ì¹˜ í¬ê¸° ê°ì†Œ: {old_size} â†’ {self.current_batch_size}")
                return

            # 2. ë©”ëª¨ë¦¬ ì—¬ìœ  ì‹œ ë°°ì¹˜ í¬ê¸° ì¦ê°€
            # ë‚¨ì€ ë©”ëª¨ë¦¬ê°€ 60% ì´ìƒì´ê³  (ë³´ìˆ˜ì  ì ‘ê·¼) í˜„ì¬ ë°°ì¹˜ê°€ ìµœëŒ€ê°€ ì•„ë‹ˆë©´ ì¦ê°€
            if free_ratio > 0.6 and self.current_batch_size < 512:
                old_size = self.current_batch_size
                self.current_batch_size = min(512, self.current_batch_size * 2)
                if old_size != self.current_batch_size:
                    self.stats['batch_size_adjustments'] += 1
                    print(f"ğŸš€ GPU ì—¬ìœ  ë©”ëª¨ë¦¬ í™•ë³´ ({free_ratio*100:.1f}%), ë°°ì¹˜ í¬ê¸° ì¦ê°€: {old_size} â†’ {self.current_batch_size}")

        except Exception as e:
            # VRAM ì¡°ì • ì‹¤íŒ¨ëŠ” ë¬´ì‹œ
            pass

    def _calculate_similarity_batch(self, frame_pairs: list) -> list:
        """
        ë°°ì¹˜ ë‹¨ìœ„ ìœ ì‚¬ë„ ê³„ì‚° (ResNet ê¸°ë°˜, GPU ìµœì í™”)

        1. ëª¨ë“  í”„ë ˆì„ì„ ë°°ì¹˜ë¡œ ë¬¶ê¸°
        2. ResNet forward pass (ë°°ì¹˜ ì²˜ë¦¬)
        3. L2 ì •ê·œí™”
        4. í˜ì–´ë³„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°

        Args:
            frame_pairs: [(img1, img2), ...] í”„ë ˆì„ ìŒ ë¦¬ìŠ¤íŠ¸

        Returns:
            ìœ ì‚¬ë„ ì ìˆ˜ ë¦¬ìŠ¤íŠ¸
        """
        if not frame_pairs:
            return []

        # Feature Extractor í•„ìˆ˜ (GPU)
        if self.feature_extractor is not None:
            similarities = self.feature_extractor.calculate_similarity_batch(frame_pairs)
            return similarities

        # GPU ì‚¬ìš© ë¶ˆê°€ ì‹œ ì˜¤ë¥˜ ë°œìƒ
        raise RuntimeError("GPU Feature Extractorë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. GPU ê°€ì†ì„ í™œì„±í™”í•´ì£¼ì„¸ìš”.")

    def _calculate_ssim_gpu(self, img1: np.ndarray, img2: np.ndarray) -> float:
        """
        GPUë¥¼ ì‚¬ìš©í•œ SSIM ê³„ì‚° (PyTorch)

        Args:
            img1: ì²« ë²ˆì§¸ ì´ë¯¸ì§€ (BGR)
            img2: ë‘ ë²ˆì§¸ ì´ë¯¸ì§€ (BGR)

        Returns:
            SSIM ì ìˆ˜
        """
        try:
            import torch
            import torch.nn.functional as F

            # BGR to Grayscale
            gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)
            gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)

            # NumPy to Torch Tensor
            t1 = torch.from_numpy(gray1).float().unsqueeze(0).unsqueeze(0).to(self.device) / 255.0
            t2 = torch.from_numpy(gray2).float().unsqueeze(0).unsqueeze(0).to(self.device) / 255.0

            # SSIM ê³„ì‚° (ê°„ë‹¨í•œ ë²„ì „)
            C1 = 0.01 ** 2
            C2 = 0.03 ** 2

            mu1 = F.avg_pool2d(t1, 11, 1, 5)
            mu2 = F.avg_pool2d(t2, 11, 1, 5)

            mu1_sq = mu1 ** 2
            mu2_sq = mu2 ** 2
            mu1_mu2 = mu1 * mu2

            sigma1_sq = F.avg_pool2d(t1 ** 2, 11, 1, 5) - mu1_sq
            sigma2_sq = F.avg_pool2d(t2 ** 2, 11, 1, 5) - mu2_sq
            sigma12 = F.avg_pool2d(t1 * t2, 11, 1, 5) - mu1_mu2

            ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / \
                       ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))

            score = ssim_map.mean().item()
            return score

        except Exception as e:
            # GPU ì˜¤ë¥˜ ì‹œ CPUë¡œ í´ë°±
            print(f"âš ï¸ GPU SSIM ê³„ì‚° ì‹¤íŒ¨, CPUë¡œ í´ë°±: {e}")
            gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)
            gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)
            score, _ = ssim(gray1, gray2, full=True)
            return score

    def detect_segments(
        self,
        video_path: Path,
        progress_callback=None
    ) -> List[VideoSegment]:
        """
        ë¹„ë””ì˜¤ì—ì„œ ìœ íš¨ êµ¬ê°„ íƒì§€ (Virtual Timeline ë°©ì‹)

        1ë‹¨ê³„: ì •ì  êµ¬ê°„ ìŠ¤ìº” ë° ë§ˆí‚¹
        2ë‹¨ê³„: ìœ íš¨ êµ¬ê°„ ë³‘í•© (ì§§ì€ ì •ì  êµ¬ê°„ ë¬´ì‹œ)
        3ë‹¨ê³„: ëˆ„ì  ì‹œê°„ ê¸°ë°˜ ì„¸ê·¸ë¨¼íŠ¸ ë¶„í• 

        Args:
            video_path: ì…ë ¥ ë¹„ë””ì˜¤ ê²½ë¡œ
            progress_callback: ì§„í–‰ ìƒí™© ì½œë°± í•¨ìˆ˜(current, total)

        Returns:
            VideoSegment ë¦¬ìŠ¤íŠ¸ (Virtual Timeline ê¸°ë°˜)
        """
        # Keyframe ì¸ë±ì‹± (í™œì„±í™” ì‹œ)
        if self.config.enable_keyframe_snap:
            print("ğŸ” Keyframe ì¸ë±ì‹± ì¤‘...")
            self.keyframes = extract_keyframes(video_path)

        # GPU Resource Manager ì‚¬ìš©
        with GPUResourceManager(self.device):
            # 1ë‹¨ê³„: ì •ì  êµ¬ê°„ íƒì§€
            static_intervals = self._scan_static_intervals(video_path, progress_callback)

            # static_intervals ì €ì¥ (ì‹œê°í™”ìš©)
            self.static_intervals_cache = static_intervals

            # 2ë‹¨ê³„: ìœ íš¨ êµ¬ê°„ ê³„ì‚°
            valid_intervals = self._compute_valid_intervals(static_intervals, video_path)

            # 3ë‹¨ê³„: Virtual Timeline ê¸°ë°˜ ì„¸ê·¸ë¨¼íŠ¸ ë¶„í• 
            segments = self._split_by_virtual_timeline(valid_intervals, video_path)

        return segments

    def _scan_static_intervals(
        self,
        video_path: Path,
        progress_callback=None
    ) -> List[Tuple[float, float]]:
        """
        1ë‹¨ê³„: ì •ì  êµ¬ê°„ ìŠ¤ìº”

        Returns:
            ì •ì  êµ¬ê°„ ë¦¬ìŠ¤íŠ¸ [(start_time, end_time), ...]
        """
        # 1ë‹¨ê³„: OpenCVë¡œ ì—´ê¸° ì‹œë„
        cap = cv2.VideoCapture(str(video_path))
        using_pyav = False

        if not cap.isOpened():
            print("âš ï¸ OpenCVë¡œ ë¹„ë””ì˜¤ë¥¼ ì—´ ìˆ˜ ì—†ì–´ PyAVë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
            cap = PyAVVideoReader(video_path)
            if not cap.open():
                raise RuntimeError(f"ë¹„ë””ì˜¤ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
            using_pyav = True

        fps = cap.get(cv2.CAP_PROP_FPS) if not using_pyav else cap.fps
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) if not using_pyav else cap.total_frames
        self.stats['total_frames'] = total_frames

        print(f"ğŸ“¹ ë¹„ë””ì˜¤ ë¶„ì„ ì¤‘...")
        print(f"   - FPS: {fps:.2f}")
        print(f"   - ì´ í”„ë ˆì„: {total_frames:,}ê°œ")
        print(f"   - ê¸¸ì´: {total_frames / fps / 60:.1f}ë¶„")
        print(f"   - Feature ìƒ˜í”Œë§: {self.config.feature_sample_rate}í”„ë ˆì„ë§ˆë‹¤")
        print(f"   - ë™ì  ë°°ì¹˜ í¬ê¸°: {self.current_batch_size} (ìë™ ì¡°ì •)")

        # 2ë‹¨ê³„: ì²« í”„ë ˆì„ ì½ê¸° ê²€ì¦
        ret, prev_frame = cap.read()
        if not ret and not using_pyav:
            print("âš ï¸ OpenCVë¡œ ì²« í”„ë ˆì„ ì½ê¸° ì‹¤íŒ¨. PyAVë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
            cap.release()
            cap = PyAVVideoReader(video_path)
            if not cap.open():
                raise RuntimeError("PyAVë¡œë„ ë¹„ë””ì˜¤ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            using_pyav = True
            fps = cap.fps
            total_frames = cap.total_frames
            self.stats['total_frames'] = total_frames
            ret, prev_frame = cap.read()

        if not ret:
            cap.release()
            raise RuntimeError("ì²« í”„ë ˆì„ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        frame_idx = 0
        static_intervals = []
        static_start = None
        consecutive_static_frames = 0

        import time
        last_update_time = time.time()
        UPDATE_INTERVAL = 0.1

        # GPU ë°°ì¹˜ ì²˜ë¦¬
        use_batch = self.feature_extractor is not None
        frame_batch = []
        frame_indices = []

        # ìœ ì‚¬ë„ ì ìˆ˜ ìºì‹œ ì´ˆê¸°í™”
        self.similarity_scores_cache = []

        # FrameReader ì‹œì‘ (I/O ë³‘ë ¬ ì²˜ë¦¬)
        reader = FrameReader(cap, queue_size=64)
        reader.start()

        try:
            while True:
                # FrameReaderì—ì„œ í”„ë ˆì„ ê°€ì ¸ì˜¤ê¸°
                ret, current_frame = reader.read()
                if not ret:
                    break

                frame_idx += 1

                # í”„ë ˆì„ ìƒ˜í”Œë§ (feature_sample_rate ì‚¬ìš©)
                if frame_idx % self.config.feature_sample_rate != 0:
                    prev_frame = current_frame
                    continue

                # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                current_time = time.time()
                if progress_callback and (current_time - last_update_time >= UPDATE_INTERVAL):
                    progress_callback(frame_idx, total_frames)
                    last_update_time = current_time

                if use_batch:
                    frame_batch.append((prev_frame.copy(), current_frame.copy()))
                    frame_indices.append(frame_idx)

                    if len(frame_batch) >= self.current_batch_size:
                        try:
                            similarity_scores = self._calculate_similarity_batch(frame_batch)

                            for batch_idx, similarity_score in enumerate(similarity_scores):
                                # ìœ ì‚¬ë„ ì ìˆ˜ ìºì‹œì— ì €ì¥ (ì‹œê°í™”ìš©)
                                self.similarity_scores_cache.append((frame_indices[batch_idx], similarity_score))

                                if similarity_score >= self.config.static_threshold:
                                    if static_start is None:
                                        static_start = (frame_indices[batch_idx] - 1) / fps
                                    consecutive_static_frames += 1
                                else:
                                    if static_start is not None and consecutive_static_frames * self.config.feature_sample_rate / fps >= self.config.min_static_duration:
                                        static_end = frame_indices[batch_idx] / fps
                                        static_intervals.append((static_start, static_end))
                                        self.stats['static_segments_removed'] += 1
                                    static_start = None
                                    consecutive_static_frames = 0

                            # ë°°ì¹˜ í¬ê¸° ë™ì  ì¡°ì •
                            self._adjust_batch_size()

                            frame_batch.clear()
                            frame_indices.clear()

                        except Exception as e:
                            try:
                                import torch
                                if isinstance(e, torch.cuda.OutOfMemoryError):
                                    self._adjust_batch_size(e)
                                    frame_batch.clear()
                                    frame_indices.clear()
                                    continue
                            except ImportError:
                                pass
                            raise

                    prev_frame = current_frame
                    continue

                # CPU ë‹¨ì¼ ì²˜ë¦¬
                similarity_score = self.calculate_similarity(prev_frame, current_frame)

                # ìœ ì‚¬ë„ ì ìˆ˜ ìºì‹œì— ì €ì¥
                self.similarity_scores_cache.append((frame_idx, similarity_score))

                if similarity_score >= self.config.static_threshold:
                    if static_start is None:
                        static_start = (frame_idx - 1) / fps
                    consecutive_static_frames += 1
                else:
                    if static_start is not None and consecutive_static_frames * self.config.feature_sample_rate / fps >= self.config.min_static_duration:
                        static_end = frame_idx / fps
                        static_intervals.append((static_start, static_end))
                        self.stats['static_segments_removed'] += 1
                    static_start = None
                    consecutive_static_frames = 0

                prev_frame = current_frame

        finally:
            # FrameReader ì •ë¦¬
            reader.stop()
            reader.join(timeout=1.0)
            cap.release()

        # ë§ˆì§€ë§‰ ì •ì  êµ¬ê°„ ì²˜ë¦¬
        if static_start is not None and consecutive_static_frames * self.config.feature_sample_rate / fps >= self.config.min_static_duration:
            static_end = frame_idx / fps
            static_intervals.append((static_start, static_end))
            self.stats['static_segments_removed'] += 1

        print(f"\nâœ… ì •ì  êµ¬ê°„ {len(static_intervals)}ê°œ íƒì§€ ì™„ë£Œ")
        print(f"   - ìœ ì‚¬ë„ ì ìˆ˜ {len(self.similarity_scores_cache)}ê°œ ìˆ˜ì§‘ë¨")
        return static_intervals

    def _compute_valid_intervals(
        self,
        static_intervals: List[Tuple[float, float]],
        video_path: Path
    ) -> List[Tuple[float, float]]:
        """
        2ë‹¨ê³„: ìœ íš¨ êµ¬ê°„ ê³„ì‚° (ì •ì  êµ¬ê°„ ì œì™¸)

        Returns:
            ìœ íš¨ êµ¬ê°„ ë¦¬ìŠ¤íŠ¸ [(start_time, end_time), ...]
        """
        cap = cv2.VideoCapture(str(video_path))
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        total_duration = total_frames / fps
        cap.release()

        if not static_intervals:
            return [(0.0, total_duration)]

        valid_intervals = []
        prev_end = 0.0

        for start, end in sorted(static_intervals):
            if start > prev_end + 0.1:  # 0.1ì´ˆ ì´ìƒ ì°¨ì´
                valid_intervals.append((prev_end, start))
            prev_end = end

        # ë§ˆì§€ë§‰ êµ¬ê°„
        if prev_end < total_duration - 0.1:
            valid_intervals.append((prev_end, total_duration))

        total_valid_duration = sum(end - start for start, end in valid_intervals)
        print(f"âœ… ìœ íš¨ êµ¬ê°„ {len(valid_intervals)}ê°œ (ì´ {total_valid_duration / 60:.1f}ë¶„)")

        return valid_intervals

    def _split_by_virtual_timeline(
        self,
        valid_intervals: List[Tuple[float, float]],
        video_path: Path
    ) -> List[VideoSegment]:
        """
        3ë‹¨ê³„: Virtual Timeline ê¸°ë°˜ ì„¸ê·¸ë¨¼íŠ¸ ë¶„í•  (ê°œì„ ëœ ë¡œì§)

        1. ëª¨ë“  ìœ íš¨ êµ¬ê°„ì— Keyframe ìŠ¤ëƒ… ì ìš©
        2. ì¤‘ë³µ/ê²¹ì¹¨ ì œê±° ë° ë³‘í•© â†’ clean_intervals
        3. clean_intervalsë¥¼ target_durationì— ë§ê²Œ ë¶„í• 
        4. ìµœì¢… ì„¸ê·¸ë¨¼íŠ¸ ìƒì„±
        """
        cap = cv2.VideoCapture(str(video_path))
        fps = cap.get(cv2.CAP_PROP_FPS)
        cap.release()

        # ===== 1ë‹¨ê³„: Keyframe ìŠ¤ëƒ… ì ìš© =====
        snapped_intervals = []
        for start, end in valid_intervals:
            if self.config.enable_keyframe_snap and self.keyframes:
                start = snap_to_keyframe(start, self.keyframes, 'before')
                end = snap_to_keyframe(end, self.keyframes, 'after')
            
            if end > start:
                snapped_intervals.append((start, end))

        if not snapped_intervals:
            return []

        # ===== 2ë‹¨ê³„: ì¤‘ë³µ ì œê±° ë° ë³‘í•© =====
        clean_intervals = self._merge_overlapping_intervals(snapped_intervals)
        
        total_clean_duration = sum(e - s for s, e in clean_intervals)
        print(f"ğŸ”§ Keyframe ìŠ¤ëƒ… í›„ êµ¬ê°„ ë³‘í•©: {len(snapped_intervals)}ê°œ â†’ {len(clean_intervals)}ê°œ (ì´ {total_clean_duration / 60:.1f}ë¶„)")

        # ===== 3ë‹¨ê³„: target_durationì— ë§ê²Œ ë¶„í•  =====
        segments = []
        accumulated_time = 0.0
        segment_intervals = []
        target = self.config.target_segment_duration

        for start, end in clean_intervals:
            current_pos = start
            
            while current_pos < end:
                remaining_for_segment = target - accumulated_time
                remaining_in_interval = end - current_pos
                
                if remaining_in_interval <= remaining_for_segment:
                    # í˜„ì¬ intervalì„ ì „ë¶€ ì‚¬ìš©
                    segment_intervals.append((current_pos, end))
                    accumulated_time += remaining_in_interval
                    current_pos = end
                else:
                    # í˜„ì¬ intervalì„ ì˜ë¼ì„œ ì„¸ê·¸ë¨¼íŠ¸ ì™„ì„±
                    cut_point = current_pos + remaining_for_segment
                    
                    # Keyframe ìŠ¤ëƒ… (ì»¤íŒ… ì§€ì )
                    if self.config.enable_keyframe_snap and self.keyframes:
                        cut_point = snap_to_keyframe(cut_point, self.keyframes, 'before')
                        # ë„ˆë¬´ ì§§ìœ¼ë©´ ë‹¤ìŒ keyframeìœ¼ë¡œ
                        if cut_point <= current_pos + 1.0:
                            cut_point = snap_to_keyframe(current_pos + remaining_for_segment, self.keyframes, 'after')
                        # endë¥¼ ë„˜ì§€ ì•Šë„ë¡
                        cut_point = min(cut_point, end)
                    
                    actual_duration = cut_point - current_pos
                    
                    if actual_duration > 0:
                        segment_intervals.append((current_pos, cut_point))
                        accumulated_time += actual_duration
                    
                    # ì„¸ê·¸ë¨¼íŠ¸ ìƒì„±
                    if segment_intervals:
                        seg_start = segment_intervals[0][0]
                        seg_end = segment_intervals[-1][1]
                        seg_duration = sum(e - s for s, e in segment_intervals)
                        
                        segments.append(VideoSegment(
                            start_frame=int(seg_start * fps),
                            end_frame=int(seg_end * fps),
                            start_time=seg_start,
                            end_time=seg_end,
                            duration=seg_duration,
                            avg_ssim=0.0,
                            intervals=list(segment_intervals)
                        ))
                    
                    # ë‹¤ìŒ ì„¸ê·¸ë¨¼íŠ¸ ì¤€ë¹„
                    current_pos = cut_point
                    accumulated_time = 0.0
                    segment_intervals = []
                
                # ë¬´í•œ ë£¨í”„ ë°©ì§€
                if current_pos >= end:
                    break

        # ë§ˆì§€ë§‰ ì„¸ê·¸ë¨¼íŠ¸
        if segment_intervals:
            seg_start = segment_intervals[0][0]
            seg_end = segment_intervals[-1][1]
            seg_duration = sum(e - s for s, e in segment_intervals)
            
            segments.append(VideoSegment(
                start_frame=int(seg_start * fps),
                end_frame=int(seg_end * fps),
                start_time=seg_start,
                end_time=seg_end,
                duration=seg_duration,
                avg_ssim=0.0,
                intervals=list(segment_intervals)
            ))

        self.stats['output_segments'] = len(segments)
        if segments:
            avg_duration = sum(s.duration for s in segments) / len(segments)
            print(f"âœ… ìµœì¢… ì„¸ê·¸ë¨¼íŠ¸ {len(segments)}ê°œ ìƒì„± (í‰ê·  {avg_duration:.1f}ì´ˆ)")
        else:
            print("âš ï¸ ìœ íš¨í•œ ì„¸ê·¸ë¨¼íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")

        return segments

    def _merge_overlapping_intervals(
        self,
        intervals: List[Tuple[float, float]]
    ) -> List[Tuple[float, float]]:
        """
        ê²¹ì¹˜ê±°ë‚˜ ì¸ì ‘í•œ êµ¬ê°„ ë³‘í•©
        """
        if not intervals:
            return []
        
        # ì¤‘ë³µ ì œê±° (ì†Œìˆ˜ì  3ìë¦¬ ë°˜ì˜¬ë¦¼)
        seen = set()
        unique = []
        for s, e in intervals:
            key = (round(s, 3), round(e, 3))
            if key not in seen:
                seen.add(key)
                unique.append((s, e))
        
        # ì •ë ¬
        unique.sort(key=lambda x: x[0])
        
        # ë³‘í•©
        merged = []
        for start, end in unique:
            if not merged:
                merged.append([start, end])
            else:
                last_start, last_end = merged[-1]
                # ê²¹ì¹˜ê±°ë‚˜ ì¸ì ‘ (0.1ì´ˆ ì´ë‚´)
                if start <= last_end + 0.1:
                    merged[-1][1] = max(last_end, end)
                else:
                    merged.append([start, end])
        
        return [(s, e) for s, e in merged]

    def _deduplicate_segment_intervals(
        self,
        segments: List[VideoSegment],
        fps: float
    ) -> List[VideoSegment]:
        """
        ì„¸ê·¸ë¨¼íŠ¸ì˜ intervals ì¤‘ë³µ ì œê±° ë° ë³‘í•©
        
        Keyframe ìŠ¤ëƒ…ìœ¼ë¡œ ì¸í•´ ê°™ì€ êµ¬ê°„ì´ ì—¬ëŸ¬ ë²ˆ í¬í•¨ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ:
        1. ê° ì„¸ê·¸ë¨¼íŠ¸ ë‚´ ì¤‘ë³µ interval ì œê±°
        2. ê²¹ì¹˜ê±°ë‚˜ ì¸ì ‘í•œ interval ë³‘í•©
        3. ë¹ˆ ì„¸ê·¸ë¨¼íŠ¸ ì œê±°
        4. duration ì¬ê³„ì‚°
        """
        cleaned_segments = []
        
        for seg in segments:
            if not seg.intervals:
                continue
            
            # 1. ì¤‘ë³µ ì œê±° (ìˆœì„œ ìœ ì§€)
            seen = set()
            unique_intervals = []
            for interval in seg.intervals:
                # íŠœí”Œì„ ì†Œìˆ˜ì  3ìë¦¬ë¡œ ë°˜ì˜¬ë¦¼í•˜ì—¬ ë¹„êµ (ë¶€ë™ì†Œìˆ˜ì  ì˜¤ì°¨ ë°©ì§€)
                key = (round(interval[0], 3), round(interval[1], 3))
                if key not in seen:
                    seen.add(key)
                    unique_intervals.append(interval)
            
            # 2. ì •ë ¬
            unique_intervals.sort(key=lambda x: x[0])
            
            # 3. ê²¹ì¹˜ê±°ë‚˜ ì¸ì ‘í•œ êµ¬ê°„ ë³‘í•©
            merged_intervals = []
            for start, end in unique_intervals:
                if not merged_intervals:
                    merged_intervals.append([start, end])
                else:
                    last_start, last_end = merged_intervals[-1]
                    # ê²¹ì¹˜ê±°ë‚˜ ì¸ì ‘í•œ ê²½ìš° (0.1ì´ˆ ì´ë‚´)
                    if start <= last_end + 0.1:
                        merged_intervals[-1][1] = max(last_end, end)
                    else:
                        merged_intervals.append([start, end])
            
            # íŠœí”Œë¡œ ë³€í™˜
            merged_intervals = [(s, e) for s, e in merged_intervals]
            
            # 4. ë¹ˆ êµ¬ê°„ ì œê±° ë° duration ì¬ê³„ì‚°
            valid_intervals = [(s, e) for s, e in merged_intervals if e > s]
            
            if not valid_intervals:
                continue
            
            # duration ì¬ê³„ì‚°
            total_duration = sum(e - s for s, e in valid_intervals)
            
            # ìƒˆ ì„¸ê·¸ë¨¼íŠ¸ ìƒì„±
            seg_start = valid_intervals[0][0]
            seg_end = valid_intervals[-1][1]
            
            cleaned_segments.append(VideoSegment(
                start_frame=int(seg_start * fps),
                end_frame=int(seg_end * fps),
                start_time=seg_start,
                end_time=seg_end,
                duration=total_duration,
                avg_ssim=seg.avg_ssim,
                intervals=valid_intervals
            ))
        
        # ì¤‘ë³µ ì œê±° ê²°ê³¼ ë¡œê¹…
        original_count = len(segments)
        cleaned_count = len(cleaned_segments)
        if original_count != cleaned_count:
            print(f"ğŸ”§ ì¤‘ë³µ ì œê±°: {original_count}ê°œ â†’ {cleaned_count}ê°œ ì„¸ê·¸ë¨¼íŠ¸")
        
        return cleaned_segments

    def _create_segment(
        self,
        start_frame: int,
        end_frame: int,
        fps: float,
        ssim_scores: List[float]
    ) -> VideoSegment:
        """ì„¸ê·¸ë¨¼íŠ¸ ê°ì²´ ìƒì„±"""
        start_time = start_frame / fps
        end_time = end_frame / fps
        duration = end_time - start_time
        avg_ssim = np.mean(ssim_scores) if ssim_scores else 0.0

        return VideoSegment(
            start_frame=start_frame,
            end_frame=end_frame,
            start_time=start_time,
            end_time=end_time,
            duration=duration,
            avg_ssim=avg_ssim
        )

    def _visualize_similarity_scores(
        self,
        output_dir: Path,
        fps: float,
        static_intervals: List[Tuple[float, float]],
        segments: List[VideoSegment]
    ):
        """
        ìœ ì‚¬ë„ ì ìˆ˜ ì‹œê°í™” (./result_seg/similarity_graph.png)

        ê·¸ë˜í”„ êµ¬ì„±:
        - Xì¶•: ì‹œê°„(ì´ˆ)
        - Yì¶•: ìœ ì‚¬ë„ ì ìˆ˜ (0~1)
        - ë¹¨ê°„ ì˜ì—­: ì •ì  êµ¬ê°„ (ì œê±°ë  ë¶€ë¶„)
        - ë…¹ìƒ‰ ì˜ì—­: ìœ íš¨ êµ¬ê°„ (ì‚¬ìš©ë  ë¶€ë¶„)
        - íŒŒë€ ì„ : ì„¸ê·¸ë¨¼íŠ¸ ê²½ê³„ (30ì´ˆ ë‹¨ìœ„)
        """
        if not self.similarity_scores_cache:
            print("âš ï¸ ìœ ì‚¬ë„ ì ìˆ˜ê°€ ì—†ì–´ ì‹œê°í™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return

        try:
            import matplotlib
            matplotlib.use('Agg')  # GUI ì—†ëŠ” í™˜ê²½ ì§€ì›
            import matplotlib.pyplot as plt

            # ë°ì´í„° ì¤€ë¹„
            frame_numbers = [frame_idx for frame_idx, _ in self.similarity_scores_cache]
            scores = [score for _, score in self.similarity_scores_cache]
            times = [frame_idx / fps for frame_idx in frame_numbers]

            # ê·¸ë˜í”„ ìƒì„±
            fig, ax = plt.subplots(figsize=(16, 6))

            # ìœ ì‚¬ë„ ì ìˆ˜ í”Œë¡¯
            ax.plot(times, scores, 'b-', linewidth=0.5, label='Cosine Similarity', alpha=0.7)

            # ì •ì  êµ¬ê°„ í‘œì‹œ (ë¹¨ê°„ ë°°ê²½)
            for start_time, end_time in static_intervals:
                ax.axvspan(start_time, end_time, color='red', alpha=0.2, label='Static Interval' if static_intervals.index((start_time, end_time)) == 0 else '')

            # ì„¸ê·¸ë¨¼íŠ¸ ê²½ê³„ í‘œì‹œ (íŒŒë€ ì„¸ë¡œì„ )
            for idx, segment in enumerate(segments):
                ax.axvline(x=segment.start_time, color='blue', linestyle='--', linewidth=1.5, alpha=0.7, label='Segment Boundary' if idx == 0 else '')

            # ì„ê³„ê°’ ì„  í‘œì‹œ
            ax.axhline(y=self.config.static_threshold, color='orange', linestyle=':', linewidth=2, label=f'Static Threshold ({self.config.static_threshold})')

            # ë ˆì´ë¸” ë° ì œëª©
            ax.set_xlabel('Time (seconds)', fontsize=12)
            ax.set_ylabel('Cosine Similarity', fontsize=12)
            ax.set_title('Video Similarity Analysis (ResNet-based)', fontsize=14, fontweight='bold')
            ax.set_ylim(0, 1.05)
            ax.grid(True, alpha=0.3)
            ax.legend(loc='upper right', fontsize=10)

            # ì €ì¥
            output_path = output_dir / 'similarity_graph.png'
            plt.tight_layout()
            plt.savefig(output_path, dpi=150)
            plt.close()

            print(f"ğŸ“Š ìœ ì‚¬ë„ ê·¸ë˜í”„ ì €ì¥: {output_path}")

        except ImportError:
            print("âš ï¸ matplotlibê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ì‹œê°í™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        except Exception as e:
            print(f"âš ï¸ ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")

    def _is_valid_segment(self, segment: VideoSegment) -> bool:
        """ì„¸ê·¸ë¨¼íŠ¸ ìœ íš¨ì„± ê²€ì¦"""
        # ìµœì†Œ ê¸¸ì´ ì²´í¬
        if segment.duration < self.config.min_duration:
            return False

        # ì •ì  êµ¬ê°„ ì²´í¬ (í‰ê·  SSIMì´ static_threshold ì´ìƒì´ë©´ ì œì™¸)
        if segment.avg_ssim >= self.config.static_threshold:
            return False

        return True

    def export_segments(
        self,
        video_path: Path,
        segments: List[VideoSegment],
        output_dir: Path,
        progress_callback=None
    ) -> List[Path]:
        """
        ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ê°œë³„ ë¹„ë””ì˜¤ íŒŒì¼ë¡œ ì €ì¥ (FFmpeg concat demuxer ì‚¬ìš©)

        Virtual Timelineì˜ ì—¬ëŸ¬ êµ¬ê°„ì„ ì´ì–´ë¶™ì—¬ ìµœì¢… ì„¸ê·¸ë¨¼íŠ¸ ìƒì„±

        Args:
            video_path: ì›ë³¸ ë¹„ë””ì˜¤ ê²½ë¡œ
            segments: ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
            progress_callback: ì§„í–‰ ìƒí™© ì½œë°±

        Returns:
            ì €ì¥ëœ ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        """
        # ì‹œê°í™” ìƒì„± (í™œì„±í™”ëœ ê²½ìš°)
        if self.config.enable_visualization:
            # FPS ê°€ì ¸ì˜¤ê¸°
            cap = cv2.VideoCapture(str(video_path))
            fps = cap.get(cv2.CAP_PROP_FPS)
            cap.release()

            # static_intervalsê°€ ìºì‹œë˜ì–´ ìˆëŠ” ê²½ìš°ì—ë§Œ ì‹œê°í™”
            static_intervals = getattr(self, 'static_intervals_cache', [])

            self._visualize_similarity_scores(
                output_dir=output_dir,
                fps=fps,
                static_intervals=static_intervals,
                segments=segments
            )

        # ffmpeg í™•ì¸ ë° ìë™ ì„¤ì¹˜
        if not check_and_install_ffmpeg():
            raise RuntimeError(
                "ffmpegë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. "
                "í„°ë¯¸ë„ì„ ì¬ì‹œì‘í•˜ê±°ë‚˜ ìˆ˜ë™ìœ¼ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”."
            )

        output_dir.mkdir(parents=True, exist_ok=True)
        saved_paths = []

        print(f"\nğŸ¬ ì„¸ê·¸ë¨¼íŠ¸ ë¹„ë””ì˜¤ ìƒì„± ì¤‘ (FFmpeg {'ì¬ì¸ì½”ë”©' if self.config.re_encode else 'concat demuxer'})...")

        for idx, segment in enumerate(segments):
            output_path = output_dir / f"segment_{idx+1:03d}.mp4"

            # ì¬ì¸ì½”ë”© ëª¨ë“œ
            if self.config.re_encode:
                # í•„í„° ë³µì¡ë„ ìƒì„± (ì—¬ëŸ¬ êµ¬ê°„ ë³‘í•© ì‹œ)
                filter_complex = ""
                inputs = []
                
                if segment.intervals:
                    # ì—¬ëŸ¬ êµ¬ê°„ì´ ìˆëŠ” ê²½ìš°
                    for i, (start, end) in enumerate(segment.intervals):
                        inputs.extend(['-ss', str(start), '-to', str(end), '-i', str(video_path)])
                        filter_complex += f"[{i}:v:0][{i}:a:0]"
                    
                    filter_complex += f"concat=n={len(segment.intervals)}:v=1:a=1[outv][outa]"
                    
                    cmd = [
                        'ffmpeg',
                        *inputs,
                        '-filter_complex', filter_complex,
                        '-map', '[outv]', '-map', '[outa]',
                        '-c:v', 'libx264',
                        '-crf', str(self.config.encode_quality),
                        '-preset', self.config.encode_preset,
                        '-c:a', 'aac',
                        '-b:a', '192k',
                        '-y',
                        str(output_path)
                    ]
                else:
                    # ë‹¨ì¼ êµ¬ê°„ (ì´ë¡ ìƒ intervalsê°€ ì—†ëŠ” ê²½ìš°ëŠ” ì—†ì–´ì•¼ í•˜ì§€ë§Œ ë°©ì–´ ì½”ë“œ)
                    cmd = [
                        'ffmpeg',
                        '-ss', str(segment.start_time),
                        '-to', str(segment.end_time),
                        '-i', str(video_path),
                        '-c:v', 'libx264',
                        '-crf', str(self.config.encode_quality),
                        '-preset', self.config.encode_preset,
                        '-c:a', 'aac',
                        '-b:a', '192k',
                        '-y',
                        str(output_path)
                    ]

                try:
                    subprocess.run(
                        cmd,
                        check=True,
                        capture_output=True,
                        text=True,
                        encoding='utf-8',
                        errors='replace',
                        creationflags=subprocess.CREATE_NO_WINDOW if os.name == 'nt' else 0
                    )
                    saved_paths.append(output_path)

                    if progress_callback:
                        progress_callback(idx + 1, len(segments))

                    print(f"   âœ“ segment_{idx+1:03d}.mp4 ({segment.duration:.1f}ì´ˆ, ì¬ì¸ì½”ë”© ì™„ë£Œ)")

                except subprocess.CalledProcessError as e:
                    print(f"   âš ï¸ segment_{idx+1:03d}.mp4 ìƒì„± ì‹¤íŒ¨: {e.stderr}")

            # ìŠ¤íŠ¸ë¦¼ ë³µì‚¬ ëª¨ë“œ (ê¸°ì¡´ ë°©ì‹)
            else:
                # ì—¬ëŸ¬ êµ¬ê°„ì„ concatí•´ì•¼ í•˜ëŠ” ê²½ìš°
                if segment.intervals and len(segment.intervals) > 1:
                    # concat demuxerìš© ì„ì‹œ íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ìƒì„±
                    concat_list_path = output_dir / f"_concat_{idx+1:03d}.txt"

                    with open(concat_list_path, 'w', encoding='utf-8') as f:
                        for interval_start, interval_end in segment.intervals:
                            # ê° êµ¬ê°„ë§ˆë‹¤ file, inpoint, outpoint ì§€ì •
                            # Windows ê²½ë¡œ ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬ (.as_posix())
                            f.write(f"file '{video_path.absolute().as_posix()}'\n")
                            f.write(f"inpoint {interval_start}\n")
                            f.write(f"outpoint {interval_end}\n")

                    # FFmpeg concat demuxer ì‹¤í–‰
                    cmd = [
                        'ffmpeg',
                        '-f', 'concat',
                        '-safe', '0',
                        '-i', str(concat_list_path),
                        '-c', 'copy',
                        '-y',
                        str(output_path)
                    ]

                    try:
                        subprocess.run(
                            cmd,
                            check=True,
                            capture_output=True,
                            text=True,
                            encoding='utf-8',
                            errors='replace',
                            creationflags=subprocess.CREATE_NO_WINDOW if os.name == 'nt' else 0
                        )
                        saved_paths.append(output_path)

                        if progress_callback:
                            progress_callback(idx + 1, len(segments))

                        print(f"   âœ“ segment_{idx+1:03d}.mp4 ({segment.duration:.1f}ì´ˆ, {len(segment.intervals)}ê°œ êµ¬ê°„ ë³‘í•©)")

                        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                        concat_list_path.unlink()

                    except subprocess.CalledProcessError as e:
                        print(f"   âš ï¸ segment_{idx+1:03d}.mp4 ìƒì„± ì‹¤íŒ¨: {e.stderr}")
                        if concat_list_path.exists():
                            concat_list_path.unlink()

                else:
                    # ë‹¨ì¼ êµ¬ê°„ì¸ ê²½ìš° ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
                    cmd = [
                        'ffmpeg',
                        '-i', str(video_path),
                        '-ss', str(segment.start_time),
                        '-to', str(segment.end_time),
                        '-c', 'copy',
                        '-y',
                        str(output_path)
                    ]

                    try:
                        subprocess.run(
                            cmd,
                            check=True,
                            capture_output=True,
                            text=True,
                            encoding='utf-8',
                            errors='replace',
                            creationflags=subprocess.CREATE_NO_WINDOW if os.name == 'nt' else 0
                        )
                        saved_paths.append(output_path)

                        if progress_callback:
                            progress_callback(idx + 1, len(segments))

                        print(f"   âœ“ segment_{idx+1:03d}.mp4 ({segment.duration:.1f}ì´ˆ)")

                    except subprocess.CalledProcessError as e:
                        print(f"   âš ï¸ segment_{idx+1:03d}.mp4 ìƒì„± ì‹¤íŒ¨: {e.stderr}")

        print(f"\nâœ… {len(saved_paths)}ê°œ ì„¸ê·¸ë¨¼íŠ¸ ì €ì¥ ì™„ë£Œ!")
        
        # ë²„ë ¤ì§„ êµ¬ê°„ ì €ì¥ (í™œì„±í™”ëœ ê²½ìš°)
        if self.config.save_discarded:
            self._export_discarded_segments(video_path, segments, output_dir)
        
        return saved_paths

    def _export_discarded_segments(
        self,
        video_path: Path,
        accepted_segments: List[VideoSegment],
        output_dir: Path
    ):
        """
        ì±„íƒë˜ì§€ ì•Šì€ êµ¬ê°„ì„ else í´ë”ì— ì €ì¥

        Args:
            video_path: ì›ë³¸ ë¹„ë””ì˜¤ ê²½ë¡œ
            accepted_segments: ì±„íƒëœ ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        """
        # ffmpeg í™•ì¸ (ì´ë¯¸ export_segmentsì—ì„œ ì²´í¬í–ˆìœ¼ë¯€ë¡œ ì¬í™•ì¸ë§Œ)
        if not check_and_install_ffmpeg():
            print("âš ï¸ ffmpegë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ì±„íƒë˜ì§€ ì•Šì€ êµ¬ê°„ ì €ì¥ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return

        # else í´ë” ìƒì„±
        else_dir = output_dir / "else"
        else_dir.mkdir(exist_ok=True)

        # ë¹„ë””ì˜¤ ì´ ê¸¸ì´ ê°€ì ¸ì˜¤ê¸°
        cap = cv2.VideoCapture(str(video_path))
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        total_duration = total_frames / fps
        cap.release()

        print(f"\nğŸ“¦ ì±„íƒë˜ì§€ ì•Šì€ êµ¬ê°„ ì €ì¥ ì¤‘...")

        # ì±„íƒëœ êµ¬ê°„ì„ ì‹œê°„ ìˆœìœ¼ë¡œ ì •ë ¬
        sorted_segments = sorted(accepted_segments, key=lambda s: s.start_time)

        # ë¹ˆ êµ¬ê°„ ì°¾ê¸°
        discarded_segments = []
        prev_end_time = 0.0

        for segment in sorted_segments:
            if segment.start_time > prev_end_time + 0.1:  # 0.1ì´ˆ ì´ìƒ ê³µë°±
                discarded_segments.append((prev_end_time, segment.start_time))
            prev_end_time = segment.end_time

        # ë§ˆì§€ë§‰ êµ¬ê°„ ì´í›„
        if prev_end_time < total_duration - 0.1:
            discarded_segments.append((prev_end_time, total_duration))

        # ë¹ˆ êµ¬ê°„ ì €ì¥
        for idx, (start_time, end_time) in enumerate(discarded_segments):
            output_path = else_dir / f"discarded_{idx+1:03d}.mp4"
            duration = end_time - start_time

            cmd = [
                'ffmpeg',
                '-i', str(video_path),
                '-ss', str(start_time),
                '-to', str(end_time),
                '-c', 'copy',
                '-y',
                str(output_path)
            ]

            try:
                subprocess.run(
                    cmd,
                    check=True,
                    capture_output=True,
                    text=True,
                    encoding='utf-8',
                    errors='replace',
                    creationflags=subprocess.CREATE_NO_WINDOW if os.name == 'nt' else 0
                )
                print(f"   âœ“ discarded_{idx+1:03d}.mp4 ({duration:.1f}ì´ˆ)")
            except subprocess.CalledProcessError as e:
                print(f"   âš ï¸ discarded_{idx+1:03d}.mp4 ìƒì„± ì‹¤íŒ¨: {e.stderr}")

        print(f"âœ… {len(discarded_segments)}ê°œ ì±„íƒë˜ì§€ ì•Šì€ êµ¬ê°„ ì €ì¥ ì™„ë£Œ!")

    def save_metadata(
        self,
        output_dir: Path,
        video_path: Path,
        segments: List[VideoSegment]
    ):
        """ë©”íƒ€ë°ì´í„° ì €ì¥"""
        metadata = {
            'source_video': str(video_path),
            'timestamp': datetime.now().isoformat(),
            'config': {
                'mode': self.config.mode,
                'static_threshold': self.config.static_threshold,
                'min_static_duration': self.config.min_static_duration,
                'target_segment_duration': self.config.target_segment_duration,
                'ssim_scale': self.config.ssim_scale,
                'frame_skip': self.config.frame_skip,
                'use_gpu': self.config.use_gpu,
                'enable_keyframe_snap': self.config.enable_keyframe_snap,
                're_encode': self.config.re_encode,
                'encode_quality': self.config.encode_quality,
            },
            'stats': self.stats,
            'segments': [
                {
                    'index': i + 1,
                    'filename': f"segment_{i+1:03d}.mp4",
                    'start_frame': seg.start_frame,
                    'end_frame': seg.end_frame,
                    'start_time': seg.start_time,
                    'end_time': seg.end_time,
                    'duration': seg.duration,
                    'avg_ssim': seg.avg_ssim,
                    'num_intervals': len(seg.intervals) if seg.intervals else 1
                }
                for i, seg in enumerate(segments)
            ]
        }

        metadata_path = output_dir / "segments_metadata.json"
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ’¾ ë©”íƒ€ë°ì´í„° ì €ì¥: {metadata_path}")

    def _print_stats(self):
        """í†µê³„ ì¶œë ¥"""
        print(f"\nğŸ“Š ì„¸ê·¸ë©˜í…Œì´ì…˜ í†µê³„:")
        print(f"   - ì´ í”„ë ˆì„: {self.stats['total_frames']:,}ê°œ")
        print(f"   - ì œê±°ëœ ì •ì  êµ¬ê°„: {self.stats['static_segments_removed']:,}ê°œ")
        print(f"   - ìµœì¢… ì¶œë ¥ ì„¸ê·¸ë¨¼íŠ¸: {self.stats['output_segments']:,}ê°œ")

        # ìœ ì‚¬ë„ ì„±ëŠ¥ í†µê³„
        gpu_count = self.stats['similarity_gpu_count']
        cpu_count = self.stats['similarity_cpu_count']
        gpu_time = self.stats['similarity_gpu_time']
        cpu_time = self.stats['similarity_cpu_time']

        if gpu_count > 0 or cpu_count > 0:
            print(f"\nâš¡ ìœ ì‚¬ë„ ê³„ì‚° ì„±ëŠ¥ í†µê³„:")
            if gpu_count > 0:
                avg_gpu_time = (gpu_time / gpu_count) * 1000  # ms
                print(f"   - GPU (ResNet): {gpu_count:,}íšŒ, í‰ê·  {avg_gpu_time:.2f}ms/í”„ë ˆì„, ì´ {gpu_time:.2f}ì´ˆ")
            if cpu_count > 0:
                avg_cpu_time = (cpu_time / cpu_count) * 1000  # ms
                print(f"   - CPU (SSIM Fallback): {cpu_count:,}íšŒ, í‰ê·  {avg_cpu_time:.2f}ms/í”„ë ˆì„, ì´ {cpu_time:.2f}ì´ˆ")
            if gpu_count > 0 and cpu_count > 0:
                speedup = (cpu_time / cpu_count) / (gpu_time / gpu_count)
                print(f"   - GPU ê°€ì† ë°°ìœ¨: {speedup:.1f}x")

        # ë°°ì¹˜ í¬ê¸° ì¡°ì • í†µê³„
        if self.stats['batch_size_adjustments'] > 0:
            print(f"\nğŸ”§ ë™ì  ë°°ì¹˜ ì¡°ì •:")
            print(f"   - ì¡°ì • íšŸìˆ˜: {self.stats['batch_size_adjustments']:,}íšŒ")
            print(f"   - ìµœì¢… ë°°ì¹˜ í¬ê¸°: {self.current_batch_size}")


def main():
    parser = argparse.ArgumentParser(
        description="SSIM ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ ë¹„ë””ì˜¤ ì„¸ê·¸ë©˜í…Œì´ì…˜"
    )
    parser.add_argument(
        '--input',
        type=Path,
        required=True,
        help="ì…ë ¥ ë¹„ë””ì˜¤ ê²½ë¡œ"
    )
    parser.add_argument(
        '--output',
        type=Path,
        required=True,
        help="ì¶œë ¥ ë””ë ‰í† ë¦¬"
    )
    parser.add_argument(
        '--mode',
        type=str,
        choices=['auto', 'custom'],
        default='auto',
        help="ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë“œ (auto=ìë™ ì„¤ì •, custom=ì‚¬ìš©ì ì§€ì •, ê¸°ë³¸: auto)"
    )
    parser.add_argument(
        '--static-threshold',
        type=float,
        default=0.95,
        help="ì •ì  êµ¬ê°„ ì„ê³„ê°’ - ì´ë³´ë‹¤ ë†’ìœ¼ë©´ ì ìˆ˜ êµ¬ê°„ìœ¼ë¡œ ì œì™¸ (ê¸°ë³¸: 0.95)"
    )
    parser.add_argument(
        '--min-static-duration',
        type=float,
        default=2.0,
        help="ìµœì†Œ ì •ì  êµ¬ê°„ ê¸¸ì´ ì´ˆ - ì´ë³´ë‹¤ ì§§ì€ ì •ì  êµ¬ê°„ì€ ë¬´ì‹œ (ê¸°ë³¸: 2.0)"
    )
    parser.add_argument(
        '--target-duration',
        type=float,
        default=600.0,
        help="ëª©í‘œ ì„¸ê·¸ë¨¼íŠ¸ ê¸¸ì´ ì´ˆ (ê¸°ë³¸: 600.0, 10ë¶„)"
    )
    parser.add_argument(
        '--ssim-scale',
        type=float,
        default=1.0,
        help="SSIM ê³„ì‚° ì‹œ í•´ìƒë„ ìŠ¤ì¼€ì¼ (0.25=4ë°° ë¹ ë¦„, 1.0=ì›ë³¸, ê¸°ë³¸: 1.0)"
    )
    parser.add_argument(
        '--frame-skip',
        type=int,
        default=1,
        help="í”„ë ˆì„ ìŠ¤í‚µ (1=ëª¨ë“  í”„ë ˆì„, 2=2í”„ë ˆì„ë§ˆë‹¤, ê¸°ë³¸: 1)"
    )
    parser.add_argument(
        '--use-gpu',
        action='store_true',
        help="GPU ê°€ì† ì‚¬ìš© (CUDA ì‚¬ìš© ê°€ëŠ¥ ì‹œ)"
    )
    parser.add_argument(
        '--disable-keyframe-snap',
        action='store_true',
        help="Keyframe ì •ë ¬ ë¹„í™œì„±í™” (ê¸°ë³¸: í™œì„±í™”)"
    )
    parser.add_argument(
        '--re-encode',
        action='store_true',
        help="ì¶œë ¥ ì‹œ ì¬ì¸ì½”ë”© ìˆ˜í–‰ (ë²„ë²…ì„ ë°©ì§€, ì†ë„ ëŠë¦¼)"
    )
    parser.add_argument(
        '--quality',
        type=int,
        default=23,
        help="ì¸ì½”ë”© í’ˆì§ˆ (CRF, 0~51, ê¸°ë³¸: 23, ë‚®ì„ìˆ˜ë¡ ê³ í™”ì§ˆ)"
    )
    parser.add_argument(
        '--preset',
        type=str,
        default='fast',
        help="ì¸ì½”ë”© í”„ë¦¬ì…‹ (ultrafast, superfast, veryfast, faster, fast, medium, slow, slower, veryslow)"
    )

    args = parser.parse_args()

    # ì„¤ì • ìƒì„±
    config = SegmentConfig(
        mode=args.mode,
        static_threshold=args.static_threshold,
        min_static_duration=args.min_static_duration,
        target_segment_duration=args.target_duration,
        ssim_scale=args.ssim_scale,
        frame_skip=args.frame_skip,
        use_gpu=args.use_gpu,
        enable_keyframe_snap=not args.disable_keyframe_snap,
        re_encode=args.re_encode,
        encode_quality=args.quality,
        encode_preset=args.preset
    )

    # ì„¸ê·¸ë©˜í„° ìƒì„±
    segmenter = VideoSegmenter(config)

    # ì§„í–‰ ìƒí™© ì½œë°±
    def progress_callback(current, total):
        print(f"   ì§„í–‰: {current:,} / {total:,} ({current / total * 100:.1f}%)", end='\r')

    try:
        # ì„¸ê·¸ë¨¼íŠ¸ íƒì§€
        segments = segmenter.detect_segments(args.input, progress_callback)

        if not segments:
            print("\nâŒ ìœ íš¨í•œ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return 1

        # ì„¸ê·¸ë¨¼íŠ¸ ë¹„ë””ì˜¤ ìƒì„±
        saved_paths = segmenter.export_segments(
            args.input,
            segments,
            args.output,
            progress_callback
        )

        # ë©”íƒ€ë°ì´í„° ì €ì¥
        segmenter.save_metadata(args.output, args.input, segments)

        print(f"\nâœ… ì™„ë£Œ!")
        print(f"   ì¶œë ¥ ë””ë ‰í† ë¦¬: {args.output}")
        print(f"   ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜: {len(saved_paths)}ê°œ")

        total_duration = sum(seg.duration for seg in segments)
        print(f"   ì´ ê¸¸ì´: {total_duration / 60:.1f}ë¶„")

    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return 1

    return 0


if __name__ == '__main__':
    exit(main())
