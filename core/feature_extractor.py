#!/usr/bin/env python3
"""
ResNet ê¸°ë°˜ Feature Extractor (GPU ìµœì í™” ë²„ì „)
ë¹„ë””ì˜¤ í”„ë ˆì„ì—ì„œ ì˜ë¯¸ì  íŠ¹ì§•ì„ ì¶”ì¶œí•˜ì—¬ ìœ ì‚¬ë„ ê³„ì‚°
- ëª¨ë“  ì—°ì‚°ì„ GPUì—ì„œ ìˆ˜í–‰
- CPU-GPU ë°ì´í„° ì „ì†¡ ìµœì†Œí™”
- ë°°ì¹˜ ìœ ì‚¬ë„ ê³„ì‚° GPU ìµœì í™”
"""

import sys
from pathlib import Path
from typing import List
import numpy as np
import cv2


class FeatureExtractor:
    """
    ResNet18 ê¸°ë°˜ Feature Extractor (GPU ìµœì í™”)

    ë¹„ë””ì˜¤ í”„ë ˆì„ â†’ 512ì°¨ì› Feature Vector ì¶”ì¶œ
    - ëª¨ë“  ì—°ì‚° GPUì—ì„œ ìˆ˜í–‰
    - L2 ì •ê·œí™” ìë™ ì ìš©
    - ë°°ì¹˜ ìœ ì‚¬ë„ ê³„ì‚° GPU ìµœì í™”
    """

    def __init__(self, device=None, use_fp16: bool = True, use_compile: bool = True):
        """
        Feature Extractor ì´ˆê¸°í™”

        Args:
            device: torch.device (Noneì´ë©´ ìë™ ê°ì§€)
            use_fp16: FP16 ì‚¬ìš© ì—¬ë¶€ (GPUì—ì„œë§Œ)
            use_compile: torch.compile() ì‚¬ìš© ì—¬ë¶€ (PyTorch 2.0+, ì„±ëŠ¥ 20-30% í–¥ìƒ)
                        ë¬¸ì œ ë°œìƒ ì‹œ Falseë¡œ ì„¤ì •í•˜ì—¬ ë¹„í™œì„±í™” ê°€ëŠ¥
        """
        self.device = device
        self.use_fp16 = use_fp16
        self.use_compile = use_compile
        self.model = None
        self.transform = None

        # ë©€í‹° ìŠ¤íŠ¸ë¦¼ íŒŒì´í”„ë¼ì´ë‹
        self.preprocessing_stream = None  # CPU ì „ì²˜ë¦¬ + GPU ì „ì†¡
        self.compute_stream = None        # GPU ì—°ì‚°

        # Pinned memory pool (CPU-GPU ì „ì†¡ ê°€ì†)
        self.pinned_memory_pool = []

        # PyTorch import ë° ëª¨ë¸ ë¡œë“œ
        self._init_model()

    def _init_model(self):
        """ResNet18 ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            self._add_pytorch_path()

            # PyTorch import ì‹œë„ (ìƒì„¸í•œ ì—ëŸ¬ ë©”ì‹œì§€)
            try:
                import torch
            except ImportError as e:
                # ë” ìƒì„¸í•œ ì—ëŸ¬ ì •ë³´ ìˆ˜ì§‘
                error_details = f"\n   - ì—ëŸ¬ ë©”ì‹œì§€: {str(e)}"
                error_details += f"\n   - sys.path ê°œìˆ˜: {len(sys.path)}"

                # PyTorch ê²½ë¡œ í™•ì¸
                pytorch_paths = [p for p in sys.path if 'pytorch' in p.lower()]
                if pytorch_paths:
                    error_details += f"\n   - PyTorch ê²½ë¡œ: {pytorch_paths[0]}"
                else:
                    error_details += "\n   - PyTorch ê²½ë¡œë¥¼ sys.pathì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŒ"

                raise RuntimeError(f"PyTorchë¥¼ importí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤:{error_details}")

            import torch.nn as nn

            # torchvision import (PyAV ì—†ì–´ë„ ì •ìƒ ì‘ë™, video readerëŠ” ì‚¬ìš© ì•ˆ í•¨)
            # PyAV ê´€ë ¨ ê²½ê³ /ì—ëŸ¬ëŠ” ë¬´ì‹œ
            import warnings
            with warnings.catch_warnings():
                warnings.filterwarnings("ignore", category=UserWarning)
                warnings.filterwarnings("ignore", category=RuntimeWarning)
                try:
                    import torchvision.models as models
                    import torchvision.transforms as T
                except Exception as e:
                    # torchvision import ì‹¤íŒ¨ ì‹œì—ë„ ê³„ì† ì§„í–‰
                    # (PyAV ë¬¸ì œì¼ ê°€ëŠ¥ì„±ì´ ë†’ì§€ë§Œ ResNet ì‚¬ìš©ì—ëŠ” ë¬¸ì œ ì—†ìŒ)
                    print(f"âš ï¸ torchvision import ê²½ê³  (ë¬´ì‹œë¨): {e}")
                    import torchvision.models as models
                    import torchvision.transforms as T

            # ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€
            if self.device is None:
                if not torch.cuda.is_available():
                    raise RuntimeError("CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. GPUê°€ í•„ìš”í•©ë‹ˆë‹¤.")
                self.device = torch.device('cuda')

            # ResNet18 ëª¨ë¸ ë¡œë“œ (fc layerë¥¼ Identityë¡œ ë³€ê²½ â†’ 512ì°¨ì› ì¶œë ¥)
            print("ğŸ”„ ResNet18 ëª¨ë¸ ë¡œë”© ì¤‘...")
            try:
                self.model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)
                print("âœ… ResNet18 weights ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                print(f"âš ï¸ ResNet18 weights ë¡œë“œ ì‹¤íŒ¨: {e}")
                print("   ê¸°ë³¸ ëª¨ë¸ë¡œ ëŒ€ì²´ ì‹œë„...")
                # weights ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ì‹œ pretrained=Falseë¡œ ì‹œë„
                self.model = models.resnet18(weights=None)
                print("âš ï¸ ì‚¬ì „ í•™ìŠµë˜ì§€ ì•Šì€ ëª¨ë¸ ì‚¬ìš© (ì •í™•ë„ ë‚®ìŒ)")

            self.model.fc = nn.Identity()
            self.model.eval()

            print(f"ğŸ”„ ëª¨ë¸ì„ GPUë¡œ ì´ë™ ì¤‘... (device: {self.device})")
            self.model = self.model.to(self.device)
            print("âœ… ëª¨ë¸ GPU ì´ë™ ì™„ë£Œ")

            # FP16 ì‚¬ìš© ì‹œ ëª¨ë¸ë„ FP16ìœ¼ë¡œ
            if self.use_fp16 and self.device.type == 'cuda':
                self.model = self.model.half()

            # torch.compile() ì‚¬ìš© (PyTorch 2.0+, 20-30% ì„±ëŠ¥ í–¥ìƒ)
            if self.use_compile and self.device.type == 'cuda':
                try:
                    # PyTorch ë²„ì „ í™•ì¸
                    torch_version = tuple(int(x) for x in torch.__version__.split('.')[:2])
                    if torch_version >= (2, 0):
                        print("ğŸ”„ torch.compile() ì ìš© ì‹œë„ ì¤‘...")
                        # mode='reduce-overhead': ì‘ì€ ëª¨ë¸ì— ìµœì í™”
                        # mode='max-autotune': ìµœëŒ€ ì„±ëŠ¥ (ì»´íŒŒì¼ ì‹œê°„ ê¹€)
                        # mode='default': ê· í˜•ì¡íŒ ì„¤ì •

                        # Triton ì„¤ì¹˜ ì—¬ë¶€ í™•ì¸
                        try:
                            import triton
                            print("   - Triton ë°œê²¬, torch.compile() í™œì„±í™”")
                            self.model = torch.compile(self.model, mode='reduce-overhead')
                            print("âœ… torch.compile() ì ìš© ì™„ë£Œ (ì„±ëŠ¥ 20-30% í–¥ìƒ ì˜ˆìƒ)")
                        except ImportError:
                            print("âš ï¸ Tritonì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ torch.compile()ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                            print("   ì¼ë°˜ ëª¨ë“œë¡œ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤. (ì„±ëŠ¥ì€ ì—¬ì „íˆ ìš°ìˆ˜í•©ë‹ˆë‹¤)")
                            print("   Triton ì„¤ì¹˜ ë°©ë²•: pip install triton")
                            self.use_compile = False  # compile ë¹„í™œì„±í™” í‘œì‹œ
                    else:
                        print(f"â„¹ï¸ PyTorch {torch.__version__}ëŠ” torch.compile()ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤ (2.0+ í•„ìš”)")
                        self.use_compile = False
                except Exception as e:
                    print(f"âš ï¸ torch.compile() ì ìš© ì‹¤íŒ¨: {e}")
                    print("   ì¼ë°˜ ëª¨ë“œë¡œ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
                    self.use_compile = False

            # ë©€í‹° CUDA ìŠ¤íŠ¸ë¦¼ ìƒì„± (íŒŒì´í”„ë¼ì´ë‹)
            if self.device.type == 'cuda':
                self.preprocessing_stream = torch.cuda.Stream()
                self.compute_stream = torch.cuda.Stream()
                print("âœ… ë©€í‹° ìŠ¤íŠ¸ë¦¼ íŒŒì´í”„ë¼ì´ë‹ í™œì„±í™”")

            # ImageNet ì •ê·œí™” íŒŒë¼ë¯¸í„° (GPU í…ì„œë¡œ ë¯¸ë¦¬ ìƒì„±)
            self.mean = torch.tensor([0.485, 0.456, 0.406], device=self.device).view(1, 3, 1, 1)
            self.std = torch.tensor([0.229, 0.224, 0.225], device=self.device).view(1, 3, 1, 1)
            if self.use_fp16:
                self.mean = self.mean.half()
                self.std = self.std.half()

            # GPU ë©”ëª¨ë¦¬ í’€ ì‚¬ì „ í• ë‹¹ (OOM ë°©ì§€)
            if self.device.type == 'cuda':
                try:
                    # ë”ë¯¸ í…ì„œë¡œ ë©”ëª¨ë¦¬ í’€ ì›Œë°ì—…
                    dummy_batch = torch.randn(32, 3, 224, 224, device=self.device)
                    if self.use_fp16:
                        dummy_batch = dummy_batch.half()
                    with torch.inference_mode():
                        _ = self.model(dummy_batch)
                    torch.cuda.synchronize()
                    del dummy_batch
                    torch.cuda.empty_cache()
                    print("âœ… GPU ë©”ëª¨ë¦¬ í’€ ì›Œë°ì—… ì™„ë£Œ")
                except Exception as e:
                    print(f"âš ï¸ GPU ë©”ëª¨ë¦¬ í’€ ì›Œë°ì—… ì‹¤íŒ¨ (ë¬´ì‹œë¨): {e}")

            print(f"âœ… ResNet18 Feature Extractor ì´ˆê¸°í™” ì™„ë£Œ")
            print(f"   - GPU: {torch.cuda.get_device_name(0)}")
            print(f"   - FP16: {'í™œì„±í™”' if self.use_fp16 else 'ë¹„í™œì„±í™”'}")
            compile_status = 'í™œì„±í™”' if self.use_compile else 'ë¹„í™œì„±í™”'
            if not self.use_compile and self.device.type == 'cuda':
                compile_status += ' (Triton ì—†ìŒ)'
            print(f"   - torch.compile(): {compile_status}")

        except ImportError as e:
            raise RuntimeError(f"PyTorchë¥¼ importí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        except Exception as e:
            raise RuntimeError(f"Feature Extractor ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    def _add_pytorch_path(self):
        """PyTorch ì„¤ì¹˜ ê²½ë¡œë¥¼ sys.pathì— ì¶”ê°€"""
        try:
            # PyTorch importì— í•„ìš”í•œ í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ëª¨ë“ˆë“¤ì„ ë¯¸ë¦¬ import
            # (PyInstaller íŒ¨í‚¤ì§• í™˜ê²½ì—ì„œ ë°œìƒí•  ìˆ˜ ìˆëŠ” import ì—ëŸ¬ ë°©ì§€)
            try:
                import modulefinder
                import importlib
                import importlib.util
                import importlib.machinery
                import pkgutil
                import inspect
            except ImportError as e:
                print(f"âš ï¸ í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ import ì‹¤íŒ¨: {e}")

            if getattr(sys, 'frozen', False):
                utils_dir = Path(sys.executable).parent / "_internal" / "src"
            else:
                script_dir = Path(__file__).parent.parent
                utils_dir = script_dir / "src"

            if utils_dir.exists() and str(utils_dir) not in sys.path:
                sys.path.insert(0, str(utils_dir))

            try:
                from utils.pytorch_installer import PyTorchInstaller
                installer = PyTorchInstaller.get_instance()
                if installer.is_pytorch_installed():
                    installer.add_to_path()
            except ImportError:
                pass
        except Exception:
            pass

    def _preprocess_frames_gpu(self, frames: List[np.ndarray], use_pinned: bool = True):
        """
        í”„ë ˆì„ì„ GPUì—ì„œ ì§ì ‘ ì „ì²˜ë¦¬ (íŒŒì´í”„ë¼ì´ë‹ ìµœì í™”)

        Args:
            frames: BGR ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸ (OpenCV í¬ë§·)
            use_pinned: Pinned memory ì‚¬ìš© ì—¬ë¶€ (CPU-GPU ì „ì†¡ ê°€ì†)

        Returns:
            ì „ì²˜ë¦¬ëœ GPU í…ì„œ (N, 3, 224, 224)
        """
        import torch
        import torch.nn.functional as F

        # BGR â†’ RGB ë³€í™˜ (CPUì—ì„œ ë¹ ë¥´ê²Œ)
        batch_np = np.stack([cv2.cvtColor(f, cv2.COLOR_BGR2RGB) for f in frames])

        # Pinned memory ì‚¬ìš© (CPU-GPU ì „ì†¡ 2-3ë°° ë¹ ë¦„)
        if use_pinned and self.device.type == 'cuda':
            # Pinned memoryì— ë³µì‚¬
            pinned_tensor = torch.from_numpy(batch_np).pin_memory()

            # ì „ì²˜ë¦¬ ìŠ¤íŠ¸ë¦¼ì—ì„œ GPUë¡œ ë¹„ë™ê¸° ì „ì†¡
            with torch.cuda.stream(self.preprocessing_stream):
                batch_tensor = pinned_tensor.to(self.device, non_blocking=True)

                # (N, H, W, C) â†’ (N, C, H, W)
                batch_tensor = batch_tensor.permute(0, 3, 1, 2)

                # FP16/FP32 ë³€í™˜ ë° ì •ê·œí™” [0, 255] â†’ [0, 1]
                if self.use_fp16:
                    batch_tensor = batch_tensor.half() / 255.0
                else:
                    batch_tensor = batch_tensor.float() / 255.0

                # ë¦¬ì‚¬ì´ì¦ˆ (224x224)
                batch_tensor = F.interpolate(batch_tensor, size=(224, 224), mode='bilinear', align_corners=False)

                # ImageNet ì •ê·œí™” (GPUì—ì„œ ì§ì ‘)
                batch_tensor = (batch_tensor - self.mean) / self.std

            # ê³„ì‚° ìŠ¤íŠ¸ë¦¼ì´ ì „ì²˜ë¦¬ ìŠ¤íŠ¸ë¦¼ì„ ê¸°ë‹¤ë¦¼
            self.compute_stream.wait_stream(self.preprocessing_stream)

        else:
            # Pinned memory ì—†ì´ ì²˜ë¦¬
            batch_tensor = torch.from_numpy(batch_np).to(self.device, non_blocking=True)
            batch_tensor = batch_tensor.permute(0, 3, 1, 2)

            if self.use_fp16:
                batch_tensor = batch_tensor.half() / 255.0
            else:
                batch_tensor = batch_tensor.float() / 255.0

            batch_tensor = F.interpolate(batch_tensor, size=(224, 224), mode='bilinear', align_corners=False)
            batch_tensor = (batch_tensor - self.mean) / self.std

        return batch_tensor

    def extract_frame_features(self, frames: List[np.ndarray]) -> np.ndarray:
        """
        í”„ë ˆì„ ë°°ì¹˜ì—ì„œ feature ì¶”ì¶œ

        Args:
            frames: BGR ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸ (OpenCV í¬ë§·)

        Returns:
            L2 ì •ê·œí™”ëœ feature ë°°ì—´ (N, 512)
        """
        import torch

        if not frames:
            return np.array([])

        try:
            # GPUì—ì„œ ì „ì²˜ë¦¬
            batch_tensor = self._preprocess_frames_gpu(frames)

            # Feature ì¶”ì¶œ
            with torch.inference_mode():
                features = self.model(batch_tensor)

                # L2 ì •ê·œí™” (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°ìš©)
                features = torch.nn.functional.normalize(features, dim=1)

                # CPUë¡œ ì´ë™ ë° numpy ë³€í™˜
                features_np = features.float().cpu().numpy()

            return features_np

        except Exception as e:
            print(f"âŒ Feature ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            raise

    def _extract_features_gpu(self, frames: List[np.ndarray]):
        """
        í”„ë ˆì„ ë°°ì¹˜ì—ì„œ feature ì¶”ì¶œ (GPU í…ì„œ ìœ ì§€, íŒŒì´í”„ë¼ì´ë‹ ìµœì í™”)

        Args:
            frames: BGR ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸

        Returns:
            L2 ì •ê·œí™”ëœ GPU í…ì„œ (N, 512)
        """
        import torch

        if not frames:
            return None

        # GPUì—ì„œ ì „ì²˜ë¦¬ (ì „ì²˜ë¦¬ ìŠ¤íŠ¸ë¦¼ ì‚¬ìš©)
        batch_tensor = self._preprocess_frames_gpu(frames)

        # Feature ì¶”ì¶œ (ê³„ì‚° ìŠ¤íŠ¸ë¦¼ì—ì„œ ì‹¤í–‰)
        with torch.cuda.stream(self.compute_stream):
            with torch.inference_mode():
                features = self.model(batch_tensor)
                # L2 ì •ê·œí™”
                features = torch.nn.functional.normalize(features, dim=1)

        # ë©”ì¸ ìŠ¤íŠ¸ë¦¼ì´ ê³„ì‚° ìŠ¤íŠ¸ë¦¼ì„ ê¸°ë‹¤ë¦¼
        torch.cuda.current_stream().wait_stream(self.compute_stream)

        return features

    def calculate_cosine_similarity(self, feat1: np.ndarray, feat2: np.ndarray) -> float:
        """
        ë‘ feature ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°

        Args:
            feat1: Feature vector 1 (512ì°¨ì›, L2 ì •ê·œí™”ë¨)
            feat2: Feature vector 2 (512ì°¨ì›, L2 ì •ê·œí™”ë¨)

        Returns:
            ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (0~1, ë†’ì„ìˆ˜ë¡ ìœ ì‚¬)
        """
        similarity = np.dot(feat1, feat2)
        return float(np.clip(similarity, 0.0, 1.0))

    def calculate_similarity_batch(self, frame_pairs: List[tuple]) -> List[float]:
        """
        ë°°ì¹˜ ë‹¨ìœ„ ìœ ì‚¬ë„ ê³„ì‚° (GPU ìµœì í™”)

        ëª¨ë“  í”„ë ˆì„ì„ í•œ ë²ˆì— GPUë¡œ ì „ì†¡í•˜ê³ ,
        ìœ ì‚¬ë„ ê³„ì‚°ë„ GPUì—ì„œ ì§ì ‘ ìˆ˜í–‰í•˜ì—¬ ë°ì´í„° ì „ì†¡ ìµœì†Œí™”

        Args:
            frame_pairs: [(frame1, frame2), ...] í”„ë ˆì„ ìŒ ë¦¬ìŠ¤íŠ¸

        Returns:
            ìœ ì‚¬ë„ ì ìˆ˜ ë¦¬ìŠ¤íŠ¸
        """
        import torch

        if not frame_pairs:
            return []

        n = len(frame_pairs)
        
        # ëª¨ë“  í”„ë ˆì„ì„ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ í¼ì¹˜ê¸° (2Nê°œ)
        all_frames = []
        for f1, f2 in frame_pairs:
            all_frames.append(f1)
            all_frames.append(f2)

        # í•œ ë²ˆì— feature ì¶”ì¶œ (GPU ìœ ì§€)
        all_features = self._extract_features_gpu(all_frames)

        if all_features is None:
            return [0.0] * n

        # ì§ìˆ˜/í™€ìˆ˜ ì¸ë±ìŠ¤ë¡œ ë¶„ë¦¬
        features1 = all_features[0::2]  # 0, 2, 4, ...
        features2 = all_features[1::2]  # 1, 3, 5, ...

        # GPUì—ì„œ ì§ì ‘ ë°°ì¹˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (ë‚´ì )
        # ì´ë¯¸ L2 ì •ê·œí™”ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ë‚´ì  = ì½”ì‚¬ì¸ ìœ ì‚¬ë„
        with torch.inference_mode():
            similarities = (features1 * features2).sum(dim=1)
            similarities = torch.clamp(similarities, 0.0, 1.0)
            similarities_list = similarities.float().cpu().tolist()

        return similarities_list

    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬ (GPU ë©”ëª¨ë¦¬ í•´ì œ)"""
        try:
            import torch
            if self.device and self.device.type == 'cuda':
                # ë©€í‹° ìŠ¤íŠ¸ë¦¼ ë™ê¸°í™”
                if self.preprocessing_stream:
                    self.preprocessing_stream.synchronize()
                    self.preprocessing_stream = None
                if self.compute_stream:
                    self.compute_stream.synchronize()
                    self.compute_stream = None

                del self.model
                del self.mean
                del self.std
                self.model = None

                # Pinned memory ì •ë¦¬
                self.pinned_memory_pool.clear()

                torch.cuda.empty_cache()
                torch.cuda.synchronize()
        except:
            pass

